---
title: "(ZFNet) Visualizing and Understanding Convolutional Networks"
date: 2025-08-28 13:00:00 +0900
categories: [AI-ML-DL, etc.]
tags: [paper, Computer-Vision, Diagnostic]
---

# (ZFNet) Visualizing and Understanding Convolutional Networks

---

### ğŸ”—Â ì¶œì²˜

> https://arxiv.org/abs/1311.2901
> 

---

### ğŸ“¦Â ì¶”ê°€ ìë£Œ

â€¦

## ğŸ§©Â ë°©ë²•ë¡ 

> **by â€¦**
> 

<aside>

</aside>

---

# ğŸ“ŒÂ ë…¼ë¬¸

## ğŸ’¡Â ìš”ì•½

> **by â€¦**
> 

<aside>

</aside>

---

## ğŸ“šÂ ì •ë¦¬

### ğŸ“ŒÂ ì œëª©

<aside>

## Visualizing and Understanding Convolutional Networks

</aside>

---

### ğŸŒŸÂ ì´ˆë¡

### ë²ˆì—­

ì›ë¬¸ (Abstract)

Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark (Krizhevsky et al., 2012). However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.

---

ë²ˆì—­ë¬¸

ëŒ€ê·œëª¨ í•©ì„±ê³± ì‹ ê²½ë§(Convolutional Network, ConvNet) ëª¨ë¸ì€ ìµœê·¼ ImageNet ë²¤ì¹˜ë§ˆí¬(Krizhevsky et al., 2012)ì—ì„œ ì¸ìƒì ì¸ ë¶„ë¥˜ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. ê·¸ëŸ¬ë‚˜ ì™œ ê·¸ë ‡ê²Œ ì˜ ì‘ë™í•˜ëŠ”ì§€, í˜¹ì€ ì–´ë–»ê²Œ ê°œì„ ë  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•´ì„œëŠ” ëª…í™•í•œ ì´í•´ê°€ ë¶€ì¡±í•˜ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ ë‘ ê°€ì§€ ë¬¸ì œë¥¼ ë‹¤ë£¬ë‹¤.

ìš°ë¦¬ëŠ” ì¤‘ê°„ íŠ¹ì§• ê³„ì¸µ(intermediate feature layers)ì˜ ê¸°ëŠ¥ê³¼ ë¶„ë¥˜ê¸°ì˜ ë™ì‘ì„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìƒˆë¡œìš´ ì‹œê°í™” ê¸°ë²•ì„ ì œì•ˆí•œë‹¤. ì§„ë‹¨ì  ë„êµ¬ë¡œ ì‚¬ìš©ë  ë•Œ, ì´ ì‹œê°í™”ëŠ” Krizhevsky et al.ì˜ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ëŠ” ì•„í‚¤í…ì²˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ë° ë„ì›€ì„ ì¤€ë‹¤. ë˜í•œ, ë‹¤ì–‘í•œ ê³„ì¸µì´ ì„±ëŠ¥ì— ê¸°ì—¬í•˜ëŠ” ì •ë„ë¥¼ ë¶„ì„í•˜ê¸° ìœ„í•´ ablation studyë¥¼ ìˆ˜í–‰í•œë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ, ìš°ë¦¬ì˜ ImageNet ëª¨ë¸ì€ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ë„ ì˜ ì¼ë°˜í™”ë¨ì„ ë³´ì˜€ë‹¤. softmax ë¶„ë¥˜ê¸°ë§Œ ìƒˆë¡œ í•™ìŠµì‹œí‚¤ë©´, Caltech-101ê³¼ Caltech-256 ë°ì´í„°ì…‹ì—ì„œ ê¸°ì¡´ ìµœê³  ì„±ëŠ¥(state-of-the-art)ì„ ëŠ¥ê°€í•˜ëŠ” ê²°ê³¼ë¥¼ ë‹¬ì„±í•œë‹¤.

<aside>

# 0. ì´ˆë¡

- ì™œ ConvNetì´ ì‘ë™í•˜ëŠ”ì§€ ëª…í™•í•œ ì´í•´ê°€ ë¶€ì¡±í•˜ë‹¤, ê·¸ë¦¬ê³  ì¶”ê°€ì ìœ¼ë¡œ ì–´ë–»ê²Œ ê°œì„ ë  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•œ ì´í•´ê°€ ë¶€ì¡±í•˜ë‹¤.
- **ì§„ë‹¨ì  ë„êµ¬** ì œì•ˆ : ì¤‘ê°„ íŠ¹ì§• ì¶”ì¶œ(intermediate feature layers)ì˜ ê¸°ëŠ¥ê³¼ ë¶„ë¥˜ê¸°ì˜ ë™ì‘ì„ ì´í•´ â†’ ê¸°ì¡´ ëª¨ë¸ì„ ëŠ¥ê°€í•˜ëŠ” ì•„í‚¤í…ì²˜ë¥¼ ì°¨ìë‚´ëŠ” ë° ë„ì›€ì„ ì¤€ë‹¤. ì¦‰ **ëˆˆìœ¼ë¡œ ì´í•´**í•  ìˆ˜ ìˆë‹¤(ë¸”ë™ë°•ìŠ¤ ë¶€ë¶„ì„ í™”ì´íŠ¸ë°•ìŠ¤ë¡œ)
- **abalation study** ìˆ˜í–‰
- **ZFNetê°œë°œ - ì „ì´ëŠ¥ë ¥** í™•ì¸

### ì •ë¦¬

| í•­ëª©      | ë‚´ìš©                                                        |
| --------- | ----------------------------------------------------------- |
| ë°ì´í„°ì…‹  | ImageNet 2012, Caltech-101, Caltech-256                     |
| ëª¨ë¸ êµ¬ì¡° | AlexNet ê¸°ë°˜, stride/filter ê°œì„ , softmax ì¶œë ¥              |
| ì—°êµ¬ ê¸°ì—¬ | Deconvnet ì‹œê°í™”, êµ¬ì¡° ìµœì í™”, ablation, ì „ì´í•™ìŠµ           |
| í‰ê°€ ê²°ê³¼ | ImageNetì—ì„œ AlexNetë³´ë‹¤ ë‚®ì€ ì˜¤ë¥˜ìœ¨, Caltechì—ì„œ SOTA ë‹¬ì„± |
</aside>

---

### ğŸ’¡Â ê²°ë¡  & ê³ ì°°

### ë²ˆì—­

ì›ë¬¸ (Discussion)

We explored large convolutional neural network models, trained for image classification, in a number ways. First, we presented a novel way to visualize the activity within the model. This reveals the features to be far from random, uninterpretable patterns. Rather, they show many intuitively desirable properties such as compositionality, increasing invariance and class discrimination as we ascend the layers. We also showed how these visualization can be used to debug problems with the model to obtain better results, for example improving on Krizhevsky et al.â€™s (Krizhevsky et al., 2012) impressive ImageNet 2012 result.

We then demonstrated through a series of occlusion experiments that the model, while trained for classification, is highly sensitive to local structure in the image and is not just using broad scene context. An ablation study on the model revealed that having a minimum depth to the network, rather than any individual section, is vital to the modelâ€™s performance.

Finally, we showed how the ImageNet trained model can generalize well to other datasets. For Caltech-101 and Caltech-256, the datasets are similar enough that we can beat the best reported results, in the latter case by a significant margin. This result brings into question to utility of benchmarks with small (i.e. < 10^4) training sets. Our convnet model generalized less well to the PASCAL data, perhaps suffering from dataset bias (Torralba & Efros, 2011), although it was still within 3.2% of the best reported result, despite no tuning for the task. For example, our performance might improve if a different loss function was used that permitted multiple objects per image. This would naturally enable the networks to tackle the object detection as well.

ë²ˆì—­ë¬¸ (ê²°ë¡  ë° ê³ ì°°)

ìš°ë¦¬ëŠ” ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•´ í•™ìŠµëœ ëŒ€ê·œëª¨ í•©ì„±ê³± ì‹ ê²½ë§(ConvNet) ëª¨ë¸ì„ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ íƒêµ¬í•˜ì˜€ë‹¤. ë¨¼ì €, ëª¨ë¸ ë‚´ë¶€ì˜ í™œë™ì„ ì‹œê°í™”í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì‹œí•˜ì˜€ë‹¤. ì´ë¥¼ í†µí•´ íŠ¹ì§•(feature)ë“¤ì´ ë¬´ì‘ìœ„ì ì´ê±°ë‚˜ í•´ì„ ë¶ˆê°€ëŠ¥í•œ íŒ¨í„´ì´ ì•„ë‹˜ì´ ë“œëŸ¬ë‚¬ë‹¤. ì˜¤íˆë ¤ ê³„ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ì¡°í•©ì„±(compositionality), ë¶ˆë³€ì„±(invariance)ì˜ ì¦ê°€, í´ë˜ìŠ¤ êµ¬ë³„(class discrimination) ë“± ì§ê´€ì ìœ¼ë¡œ ë°”ëŒì§í•œ íŠ¹ì„±ì„ ë³´ì˜€ë‹¤. ë˜í•œ ì´ëŸ¬í•œ ì‹œê°í™” ê¸°ë²•ì´ ëª¨ë¸ ë””ë²„ê¹…ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìœ¼ë©°, ì˜ˆë¥¼ ë“¤ì–´ Krizhevsky et al.(2012)ì˜ ImageNet 2012 ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆì—ˆë‹¤.

ê·¸ ë‹¤ìŒ, ì¼ë ¨ì˜ occlusion(ê°€ë¦¼) ì‹¤í—˜ì„ í†µí•´, ë¶„ë¥˜ìš©ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì´ ë‹¨ìˆœíˆ ì¥ë©´ì˜ ê´‘ë²”ìœ„í•œ ë§¥ë½(scene context)ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì´ë¯¸ì§€ì˜ **êµ­ì†Œì  êµ¬ì¡°(local structure)**ì— ë§¤ìš° ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•œë‹¤ëŠ” ì ì„ ì…ì¦í•˜ì˜€ë‹¤. ëª¨ë¸ì— ëŒ€í•œ ablation studyëŠ” ê°œë³„ ê³„ì¸µì´ ì•„ë‹ˆë¼ ë„¤íŠ¸ì›Œí¬ê°€ ê°€ì§€ëŠ” ìµœì†Œí•œì˜ ê¹Šì´(minimum depth)ê°€ ì„±ëŠ¥ì— í•„ìˆ˜ì ì„ì„ ë“œëŸ¬ëƒˆë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ, ImageNetì—ì„œ í•™ìŠµëœ ëª¨ë¸ì´ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ì„œë„ ì˜ ì¼ë°˜í™”ë¨ì„ ë³´ì˜€ë‹¤. Caltech-101ê³¼ Caltech-256ì˜ ê²½ìš° ë°ì´í„°ì…‹ íŠ¹ì„±ì´ ì¶©ë¶„íˆ ìœ ì‚¬í•˜ì—¬ ê¸°ì¡´ ìµœê³  ì„±ëŠ¥(state-of-the-art)ì„ ëŠ¥ê°€í–ˆìœ¼ë©°, íŠ¹íˆ Caltech-256ì—ì„œëŠ” ìƒë‹¹í•œ ì°¨ì´ë¡œ ì„±ëŠ¥ì„ ì•ì„°ë‹¤. ì´ ê²°ê³¼ëŠ” ì‘ì€ ê·œëª¨(<10^4)ì˜ í•™ìŠµì…‹ì„ ê°–ëŠ” ë²¤ì¹˜ë§ˆí¬ì˜ íš¨ìš©ì„±ì— ì˜ë¬¸ì„ ì œê¸°í•œë‹¤. ë°˜ë©´ PASCAL ë°ì´í„°ì—ì„œëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì¼ë°˜í™”ê°€ ì•½í–ˆëŠ”ë°, ì´ëŠ” ë°ì´í„°ì…‹ ë°”ì´ì–´ìŠ¤(dataset bias) ë•Œë¬¸ì¼ ìˆ˜ ìˆë‹¤(Torralba & Efros, 2011). ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ë³„ë„ì˜ ì¡°ì • ì—†ì´ë„ ìµœê³  ê²°ê³¼ì™€ 3.2% ì°¨ì´ì— ë¶ˆê³¼í•˜ì˜€ë‹¤. ë§Œì•½ ì´ë¯¸ì§€ ë‚´ ë‹¤ì¤‘ ê°ì²´ë¥¼ í—ˆìš©í•˜ëŠ” ë‹¤ë¥¸ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤ë©´ ì„±ëŠ¥ì€ ë” í–¥ìƒë  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ê³§ ê°ì²´ íƒì§€(object detection) ë¬¸ì œê¹Œì§€ë„ ìì—°ìŠ¤ëŸ½ê²Œ í™•ì¥ë  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.

<aside>

# 6. Disccusion

- ì‹œê°í™”í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì‹œí•¨ìœ¼ë¡œì„œ, featureë“¤ì´ ë¬´ì‘ìœ„ì ì´ê±°ë‚˜ í•´ì„ ë¶ˆê°€ëŠ¥í•œ íŒ¨í„´ì´ ì•„ë‹˜ì´ ë“¤ì–´ë‚¬ìŒ
    - ê³„ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ compositionality(ì¡°í•©ì„±), invariance(ë¶ˆë³€ì„±), class discrimination(í´ë˜ìŠ¤ êµ¬ë³„) ë“± ì§ê´€ì ì¸ íŠ¹ì„±ë“¤ì´ ë³´ì˜€ë‹¤.
    - ì‹œê°í™” ê¸°ë²•ì´ ëª¨ë¸ ë””ë²„ê¹…ì— ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤. AlexNet ì„±ëŠ¥ì„ í–¥ìƒ
- Occlusion(ê°€ë¦¼) ì‹¤í—˜ì„ í†µí•´, clf.ê°€ scene context(ì¥ë©´ì˜ ê´‘ë²”ìœ„í•œ ë§¥ë½)ì„ ì‚¬ìš©í•˜ëŠ”ê²ƒì´ ì•„ë‹ˆë¼, ì´ë¯¸ì§€ì˜ local structure(êµ­ì†Œì  êµ¬ì¡°)ì— ë§¤ìš° ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•œë‹¤ëŠ” ê²ƒì„ ì…ì¦í•¨.
- Ablation studyë¥¼ í†µí•´ ê°œë³„ ê³„ì¸µì´ ì•„ë‹ˆë¼ ë„¤íŠ¸ì›Œí¬ê°€ ê°€ì§€ëŠ” minimum depth(ìµœì†Œí•œì˜ ê¹Šì´)ê°€ ì„±ëŠ¥ì— í•„ìˆ˜ì ì´ë‹¤.
- ì „ì´ í•™ìŠµì˜ íš¨ìš©ì„±ì„ ì¦ëª…í–ˆë‹¤(ì¼ë°˜í™” ì„±ëŠ¥), ê·¸ëŸ¬ë‚˜ PASCALê³¼ ê°™ì€ ë°ì´í„°ì—ì„œ dataset biasë•Œë¬¸ì—, ì¼ë°˜í™”ê°€ ì•½í–ˆìœ¼ë‚˜ ê·¸ ë§ˆì €ë„ ë‚®ì€ ê°ì†Œì˜€ë‹¤. â†’ ê°ì²´íƒì§€ê¹Œì§€ ìì—°ìŠ¤ëŸ½ê²Œ í™•ì¥ë  ìˆ˜ ìˆë‹¤.

### í•µì‹¬

- convnetì€ í•´ì„ê°€ëŠ¥í•˜ê³ , ê³„ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ì¶”ìƒí™”, ë¶ˆë³€ì„±ì´ ì¦ê°€
- ì‹œê°í™” : ë‹¨ìˆœ ì‹œê° ì„¤ëª…ë„êµ¬ê°€ ì•„ë‹ˆë¼, ëª¨ë¸ ë””ë²„ê¹…(ê°œì„ ìš©)ì— ì‚¬ìš©ê°€ëŠ¥
- Occlusion : ëª¨ë¸ì€ ë§¥ë½ë³´ë‹¤, ì§„ì§œ ê°ì²´ êµ¬ì¡°(local structure)ì— ì§‘ì¤‘
- Ablation : ëª¨ë¸ì€ ì¸µì˜ ë‰´ëŸ°ìˆ˜ë³´ë‹¤, ì¶©ë¶„í•œ ê¹Šì´ê°€ ì„±ëŠ¥ì— í•µìŠ´
- ì „ì´í•™ìŠµ íŠ¹ì • ë°ì´í„°ì…‹ì—ì„œ íš¨ê³¼ì , ê·¸ëŸ¬ë‚˜ ë°ì´í„°ì˜ biasë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤ë©´ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ì„œë„ íš¨ê³¼ì ì¼ê±°ë¼ ì˜ˆìƒ
    - ImageNet â†’ Caltech(ì‘ì€ ë°ì´í„°ì…‹) : íš¨ê³¼ì 
    - ì—­ì€ íš¨ê³¼ì ì¸ì§€ ì˜ë¬¸
    - ëŒ€ê·œëª¨ë°ì´í„°ì…‹ì´ë”ë¼ë„ ëª¨ë“ ë„ë©”ì¸ì— ì™„ì „íˆ ì „ì´ë˜ì§€ ì•ŠìŒ
        
        â†’ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì†ë³¸ë‹¤ë©´ ë” í–¥ìƒë˜ì§€ ì•Šì„ê¹Œ?
        

### 5ì¤„ ìš”ì•½

- ConvNet íŠ¹ì§•ì€ ë¬´ì‘ìœ„ê°€ ì•„ë‹ˆë¼ ì ì§„ì ìœ¼ë¡œ ì¶”ìƒí™”ë˜ëŠ” ì˜ë¯¸ ìˆëŠ” í‘œí˜„
- Deconvnet ì‹œê°í™”ëŠ” ëª¨ë¸ ì„±ëŠ¥ ê°œì„ ì— ìœ ìš©í•œ ì§„ë‹¨ ë„êµ¬
- ëª¨ë¸ì€ êµ­ì†Œì  êµ¬ì¡°ë¥¼ ì˜ ì¡ì•„ë‚´ë©° ê¹Šì´ê°€ í•„ìˆ˜ì ì„ì„ í™•ì¸
- ImageNet í•™ìŠµ ëª¨ë¸ì€ Caltech ê³„ì—´ì—ì„œ SOTA ë‹¬ì„±, ì‘ì€ ë°ì´í„°ì…‹ ë²¤ì¹˜ë§ˆí¬ì˜ í•œê³„ ì œê¸°
- PASCALì—ì„œëŠ” ì¼ë°˜í™”ê°€ ì œí•œì  â†’ dataset bias, loss function ê°œì„  í•„ìš”

### ì •ë¦¬

| í•­ëª©        | ë‚´ìš©                                                                       |
| ----------- | -------------------------------------------------------------------------- |
| ì‹œê°í™” ë°œê²¬ | íŠ¹ì§•ì€ ì¶”ìƒí™”Â·ë¶ˆë³€ì„±Â·í´ë˜ìŠ¤ êµ¬ë³„ì„ ì ì°¨ ê°•í™”                               |
| Occlusion   | ê°ì²´ ìœ„ì¹˜ì— ë¯¼ê° â†’ ë°°ê²½ ë§¥ë½ë§Œ ì´ìš©í•˜ì§€ ì•ŠìŒ                               |
| Ablation    | íŠ¹ì • ì¸µë³´ë‹¤ ê¹Šì´(depth) ìì²´ê°€ í•µì‹¬                                        |
| ì „ì´ ì„±ëŠ¥   | Caltech-101/256ì—ì„œ SOTA, PASCALì€ dataset biasë¡œ ë‹¤ì†Œ ì €í•˜                |
| ì‹œì‚¬ì       | ì‘ì€ ë²¤ì¹˜ë§ˆí¬ì˜ ìœ íš¨ì„± ì¬ê²€í† , loss function ê°œì„  ì‹œ ê°ì²´ íƒì§€ë¡œ í™•ì¥ ê°€ëŠ¥ |

</aside>

---

### ğŸ—ƒï¸Â ë°ì´í„°

### ë²ˆì—­

**ImageNet 2012**

This dataset consists of 1.3M/50k/100k training/validation/test examples, spread over 1000 categories.

**Caltech-101**

We follow the procedure of (Fei-fei et al., 2006) and randomly select 15 or 30 images per class for training and test on up to 50 images per class reporting the average of the per-class accuracies in Table 4.

**Caltech-256**

We follow the procedure of (Griffin et al., 2006), selecting 15, 30, 45, or 60 training images per class, reporting the average of the per-class accuracies in Table 5.

**PASCAL VOC 2012**

We used the standard training and validation images to train a 20-way softmax on top of the ImageNet-pretrained convnet. This is not ideal, as PASCAL images can contain multiple objects and our model just provides a single exclusive prediction for each image.

---

ë²ˆì—­ë¬¸

**ImageNet 2012**

ì´ ë°ì´í„°ì…‹ì€ 1000ê°œì˜ ì¹´í…Œê³ ë¦¬ì— ê±¸ì³ 130ë§Œ ì¥ì˜ í•™ìŠµ ì´ë¯¸ì§€, 5ë§Œ ì¥ì˜ ê²€ì¦ ì´ë¯¸ì§€, 10ë§Œ ì¥ì˜ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ëœë‹¤.

**Caltech-101**

(Fei-fei et al., 2006)ì˜ ì ˆì°¨ë¥¼ ë”°ë¼ ê° í´ë˜ìŠ¤ë‹¹ 15ì¥ ë˜ëŠ” 30ì¥ì„ ë¬´ì‘ìœ„ë¡œ í•™ìŠµì— ì‚¬ìš©í•˜ê³ , ìµœëŒ€ 50ì¥ì„ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•œë‹¤. ê²°ê³¼ëŠ” í´ë˜ìŠ¤ë³„ ì •í™•ë„ì˜ í‰ê· ìœ¼ë¡œ ë³´ê³ í•œë‹¤.

**Caltech-256**

(Griffin et al., 2006)ì˜ ì ˆì°¨ë¥¼ ë”°ë¼ ê° í´ë˜ìŠ¤ë‹¹ 15, 30, 45, 60ì¥ì˜ ì´ë¯¸ì§€ë¥¼ í•™ìŠµì— ì‚¬ìš©í•˜ê³ , í´ë˜ìŠ¤ë³„ ì •í™•ë„ì˜ í‰ê· ì„ ë³´ê³ í•œë‹¤.

**PASCAL VOC 2012**

í‘œì¤€ í•™ìŠµ ë° ê²€ì¦ ì´ë¯¸ì§€ë¥¼ ì´ìš©í•´, ImageNetì—ì„œ ì‚¬ì „ í•™ìŠµëœ ConvNet ìœ„ì— 20í´ë˜ìŠ¤ softmax ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµì‹œì¼°ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ì ‘ê·¼ì€ ì™„ë²½í•˜ì§€ ì•Šì€ë°, PASCAL ì´ë¯¸ì§€ëŠ” ë‹¤ì¤‘ ê°ì²´ë¥¼ í¬í•¨í•  ìˆ˜ ìˆì§€ë§Œ ëª¨ë¸ì€ ë‹¨ì¼ ê°ì²´ ë¶„ë¥˜ë§Œ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

<aside>

## ë°ì´í„°ì…‹(ìƒëµ)

| ë°ì´í„°ì…‹        | í¬ê¸°/êµ¬ì„±                                        | íŠ¹ì§•                      | í™œìš© ëª©ì                  |
| --------------- | ------------------------------------------------ | ------------------------- | ------------------------- |
| ImageNet 2012   | 130ë§Œ í•™ìŠµ / 5ë§Œ ê²€ì¦ / 10ë§Œ í…ŒìŠ¤íŠ¸, 1000 í´ë˜ìŠ¤ | ëŒ€ê·œëª¨, ê°ì²´ ì¤‘ì‹¬         | ConvNet í•™ìŠµ ë° ì„±ëŠ¥ í‰ê°€ |
| Caltech-101     | 101 í´ë˜ìŠ¤, í´ë˜ìŠ¤ë‹¹ 15~30 í•™ìŠµ, ìµœëŒ€ 50 í…ŒìŠ¤íŠ¸  | ì†Œê·œëª¨, ë‹¨ìˆœ ê°ì²´         | ì „ì´í•™ìŠµ íš¨ê³¼ ê²€ì¦        |
| Caltech-256     | 256 í´ë˜ìŠ¤, í´ë˜ìŠ¤ë‹¹ 15~60 í•™ìŠµ                  | í´ë˜ìŠ¤ ìˆ˜ ë§ê³  ë‹¤ì–‘ì„± í¼  | ì „ì´í•™ìŠµ ê°•ê±´ì„± í‰ê°€      |
| PASCAL VOC 2012 | 20 í´ë˜ìŠ¤, ì¥ë©´ ë‚´ ë‹¤ì¤‘ ê°ì²´ í¬í•¨                | ë³µì¡í•œ ì¥ë©´, multi-object | ConvNet ì¼ë°˜í™” í•œê³„ í™•ì¸  |
</aside>

---

### ğŸ“ŒÂ ì„œë¡ 

### ë²ˆì—­

ì›ë¬¸ (Introduction ë°œì·Œ)

Since their introduction by (LeCun et al., 1989) in the early 1990â€™s, Convolutional Networks (convnets) have demonstrated excellent performance at tasks such as hand-written digit classification and face detection. In the last year, several papers have shown that they can also deliver outstanding performance on more challenging visual classification tasks. (Ciresan et al., 2012) demonstrate state-of-the-art performance on NORB and CIFAR-10 datasets. Most notably, (Krizhevsky et al., 2012) show record beating performance on the ImageNet 2012 classification benchmark, with their convnet model achieving an error rate of 16.4%, compared to the 2nd place result of 26.1%.

Several factors are responsible for this renewed interest in convnet models: (i) the availability of much larger training sets, with millions of labeled examples; (ii) powerful GPU implementations, making the training of very large models practical and (iii) better model regularization strategies, such as Dropout (Hinton et al., 2012).

Despite this encouraging progress, there is still little insight into the internal operation and behavior of these complex models, or how they achieve such good performance. From a scientific standpoint, this is deeply unsatisfactory. Without clear understanding of how and why they work, the development of better models is reduced to trial-and-error. In this paper we introduce a visualization technique that reveals the input stimuli that excite individual feature maps at any layer in the model. It also allows us to observe the evolution of features during training and to diagnose potential problems with the model. The visualization technique we propose uses a multi-layered Deconvolutional Network (deconvnet), as proposed by (Zeiler et al., 2011), to project the feature activations back to the input pixel space. We also perform a sensitivity analysis of the classifier output by occluding portions of the input image, revealing which parts of the scene are important for classification.

Using these tools, we start with the architecture of (Krizhevsky et al., 2012) and explore different architectures, discovering ones that outperform their results on ImageNet. We then explore the generalization ability of the model to other datasets, just retraining the softmax classifier on top. As such, this is a form of supervised pre-training, which contrasts with the unsupervised pre-training methods popularized by (Hinton et al., 2006) and others (Bengio et al., 2007; Vincent et al., 2008). The generalization ability of convnet features is also explored in concurrent work by (Donahue et al., 2013).

---

ë²ˆì—­ë¬¸ (ì„œë¡ )

í•©ì„±ê³± ì‹ ê²½ë§(Convolutional Networks, ConvNets)ì€ (LeCun et al., 1989)ì— ì˜í•´ 1990ë…„ëŒ€ ì´ˆ ì²˜ìŒ ì œì•ˆëœ ì´í›„, ì†ê¸€ì”¨ ìˆ«ì ë¶„ë¥˜ ë° ì–¼êµ´ ê²€ì¶œê³¼ ê°™ì€ ê³¼ì œì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì™”ë‹¤. ìµœê·¼ ëª‡ ë…„ê°„, ConvNetì€ ë” ì–´ë ¤ìš´ ì‹œê°ì  ë¶„ë¥˜ ê³¼ì œì—ì„œë„ íƒì›”í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•œë‹¤ëŠ” ê²ƒì´ ë³´ê³ ë˜ì—ˆë‹¤. (Ciresan et al., 2012)ì€ NORBì™€ CIFAR-10 ë°ì´í„°ì…‹ì—ì„œ ìµœì‹ (state-of-the-art) ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìœ¼ë©°, íŠ¹íˆ (Krizhevsky et al., 2012)ì€ ImageNet 2012 ë¶„ë¥˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ íšê¸°ì ì¸ ê¸°ë¡ì„ ì„¸ìš°ë©°, ì˜¤ë¥˜ìœ¨ 16.4%ë¥¼ ë‹¬ì„±í•˜ì˜€ë‹¤(2ìœ„ ê²°ê³¼ëŠ” 26.1%).

ConvNet ëª¨ë¸ì— ëŒ€í•œ ê´€ì‹¬ì´ ë‹¤ì‹œ ë†’ì•„ì§„ ì´ìœ ëŠ” ì—¬ëŸ¬ ê°€ì§€ ìš”ì¸ ë•Œë¬¸ì´ë‹¤: (i) ìˆ˜ë°±ë§Œ ê°œì˜ ë¼ë²¨ì´ ë‹¬ë¦° ëŒ€ê·œëª¨ í•™ìŠµ ë°ì´í„°ì…‹ì˜ ì´ìš© ê°€ëŠ¥ì„±, (ii) ëŒ€ê·œëª¨ ëª¨ë¸ í•™ìŠµì„ ê°€ëŠ¥ì¼€ í•œ ê°•ë ¥í•œ GPU êµ¬í˜„, (iii) Dropout(Hinton et al., 2012)ê³¼ ê°™ì€ í–¥ìƒëœ ì •ê·œí™” ê¸°ë²•ì´ë‹¤.

ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ê³ ë¬´ì ì¸ ì„±ê³¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ëŸ¬í•œ ë³µì¡í•œ ëª¨ë¸ì´ ë‚´ë¶€ì ìœ¼ë¡œ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€, ì™œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ”ì§€ì— ëŒ€í•´ì„œëŠ” ê±°ì˜ ì•Œë ¤ì§€ì§€ ì•Šì•˜ë‹¤. ê³¼í•™ì  ê´€ì ì—ì„œ ì´ëŠ” ë¶ˆë§Œì¡±ìŠ¤ëŸ½ë‹¤. ëª…í™•í•œ ì´í•´ê°€ ì—†ì´ëŠ” ë” ë‚˜ì€ ëª¨ë¸ ê°œë°œì´ ë‹¨ìˆœí•œ ì‹œí–‰ì°©ì˜¤(trial-and-error)ì— ì˜ì¡´í•  ìˆ˜ë°–ì— ì—†ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ëª¨ë¸ì˜ ì–´ë–¤ ê³„ì¸µì—ì„œë„ íŠ¹ì • feature mapì„ ìê·¹í•˜ëŠ” ì…ë ¥ ìê·¹(input stimuli)ì„ ì‹œê°í™”í•  ìˆ˜ ìˆëŠ” ê¸°ë²•ì„ ì œì•ˆí•œë‹¤. ë˜í•œ í›ˆë ¨ ì¤‘ íŠ¹ì§•ì´ ì–´ë–»ê²Œ ì§„í™”í•˜ëŠ”ì§€ë¥¼ ê´€ì°°í•˜ê³ , ëª¨ë¸ì˜ ì ì¬ì  ë¬¸ì œë¥¼ ì§„ë‹¨í•  ìˆ˜ ìˆë‹¤. ìš°ë¦¬ê°€ ì œì•ˆí•˜ëŠ” ì‹œê°í™” ê¸°ë²•ì€ (Zeiler et al., 2011)ì´ ì œì•ˆí•œ ë‹¤ì¸µ Deconvolutional Network(deconvnet)ë¥¼ í™œìš©í•˜ì—¬, feature activationì„ ë‹¤ì‹œ ì…ë ¥ í”½ì…€ ê³µê°„ìœ¼ë¡œ íˆ¬ì˜í•œë‹¤. ë˜í•œ ì…ë ¥ ì´ë¯¸ì§€ì˜ ì¼ë¶€ë¥¼ ê°€ë ¤ì„œ(occlusion) ë¶„ë¥˜ ì¶œë ¥ì˜ ë¯¼ê°ë„(sensitivity)ë¥¼ ë¶„ì„í•¨ìœ¼ë¡œì¨, ì¥ë©´ì˜ ì–´ë–¤ ë¶€ë¶„ì´ ë¶„ë¥˜ì— ì¤‘ìš”í•œì§€ í™•ì¸í•œë‹¤.

ì´ëŸ¬í•œ ë„êµ¬ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ìš°ë¦¬ëŠ” (Krizhevsky et al., 2012)ì˜ ì•„í‚¤í…ì²˜ë¥¼ ì¶œë°œì ìœ¼ë¡œ í•˜ì—¬ ë‹¤ì–‘í•œ êµ¬ì¡°ë¥¼ íƒìƒ‰í•˜ê³ , ImageNetì—ì„œ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë‚´ëŠ” ì•„í‚¤í…ì²˜ë¥¼ ë°œê²¬í•˜ì˜€ë‹¤. ë˜í•œ softmax ë¶„ë¥˜ê¸°ë§Œ ìƒˆë¡œ í•™ìŠµì‹œí‚¤ëŠ” ë°©ì‹ìœ¼ë¡œ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ íƒêµ¬í•˜ì˜€ë‹¤. ì´ëŠ” (Hinton et al., 2006; Bengio et al., 2007; Vincent et al., 2008) ë“±ì´ ì œì•ˆí•œ ë¹„ì§€ë„ ì‚¬ì „í•™ìŠµ(unsupervised pre-training)ê³¼ ë‹¬ë¦¬ **ì§€ë„ ì‚¬ì „í•™ìŠµ(supervised pre-training)**ì— í•´ë‹¹í•œë‹¤. ConvNet íŠ¹ì§•ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì€ (Donahue et al., 2013)ì˜ ë™ì‹œëŒ€ ì—°êµ¬ì—ì„œë„ íƒêµ¬ë˜ê³  ìˆë‹¤.

<aside>

# 1. ì„œë¡ 

- 1990ë…„ ì´ˆ ì²˜ìŒ ì œì•ˆëœ CNNì€ AlexNet(2021)ë¶€í„° íšê¸°ì ì¸ ëª¨ë¸ë¡œ ë°œì „í•´ ì™”ë‹¤. ê·¸ ì´ìœ ëŠ”
    - ëŒ€ê·œëª¨ í•™ìŠµ ë°ì´í„°ì…‹ì˜ ì´ìš© ê°€ëŠ¥ì„±
    - GPUì˜ êµ¬í˜„
    - ì •ê·œí™” ê¸°ë²•(Dropout, etc.)
- ê·¸ëŸ¬ë‚˜ Blackbox(ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜ ë¶ˆëª…í™•)ì´ê¸° ë•Œë¬¸ì—, ë” ë‚˜ì€ ëª¨ë¸ ê°œë°œì´ ë‹¨ìˆœí•œ ì‹œí–‰ì°©ì˜¤ì— ì˜ì¡´í•  ìˆ˜ ë°–ì— ì—†ë‹¤. ê·¸ë˜ì„œ ì–´ë–¤ ê³„ì¸µì—ì„œ feature mapì„ ì‹œê°í™”í•˜ëŠ” ê¸°ë²•ì„ ì œê³µí•˜ê³ , ì´ëŠ” ì–´ë–¤ íŠ¹ì§•ì´ ì–´ë–»ê²Œ ì§„í™”í•˜ëŠ”ì§€ ê´€ì°°í•˜ê³  ëª¨ë¸ì„ ì§„ë‹¨í•  ìˆ˜ ìˆë‹¤.
- Deconvolutional Network(deconvnet, (Zeiler et al., 2011))ì„ í™œìš©í•˜ì—¬, feature activationì„ ë‹¤ì‹œ í”¼ì„¸ ê³µê°„ìœ¼ë¡œ íˆ¬ì˜í•œë‹¤. ë˜ ì´ë¯¸ì§€ì˜ ì¼ë¶€ë¥¼ ê°€ë ¤ì„œ ë¶„ë¥˜ì˜ ë¯¼ê°ë„ë¥¼ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ë¶€ë¶„ì´ ë¶„ë¥˜ì— ì¤‘ìš”í•œì§€ í™•ì¸í•œë‹¤.
- ì´ëŸ¬í•œ ê¸°ë²•ë“¤ì„ í™œìš©í•˜ì—¬ AlexNetì—ì„œ ì¢€ ë” ë°œì „í•œ ZFNetì„ ë§Œë“¤ì—ˆë‹¤.

### âœ… 5ì¤„ ìš”ì•½

- ConvNetì€ ìµœê·¼ ImageNet ë“±ì—ì„œ ì„±ëŠ¥ì„ í˜ì‹ ì ìœ¼ë¡œ ê°œì„ 
- ê·¸ëŸ¬ë‚˜ ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜ì€ ì—¬ì „íˆ ë¶ˆëª…í™•
- ë³¸ ë…¼ë¬¸ì€ deconvnet ê¸°ë°˜ ì‹œê°í™”ë¡œ ì´ë¥¼ ë¶„ì„
- ëª¨ë¸ êµ¬ì¡° ê°œì„  ë° ì§„ë‹¨ ê°€ëŠ¥ì„±ì„ ì œì‹œ
- ImageNet í•™ìŠµ íŠ¹ì§•ì´ ì „ì´í•™ìŠµì—ì„œë„ íƒì›”í•¨ì„ ë³´ì„

### ğŸ“ŒÂ ì •ë¦¬

| í•­ëª©          | ë‚´ìš©                                                                                      |
| ------------- | ----------------------------------------------------------------------------------------- |
| ë°°ê²½          | ConvNet ì„±ëŠ¥ ê¸‰ìƒìŠ¹ (CIFAR-10, ImageNet ë“±)                                               |
| í•œê³„          | ë‚´ë¶€ ë™ì‘ ì›ë¦¬ì— ëŒ€í•œ ì´í•´ ë¶€ì¡±                                                           |
| ê¸°ì—¬          | ì‹œê°í™” ê¸°ë²• ì œì•ˆ (deconvnet, occlusion)                                                   |
| ì—°êµ¬ ì „ëµ     | AlexNet êµ¬ì¡° â†’ ê°œì„  â†’ ì‹œê°í™” ê¸°ë°˜ ì§„ë‹¨ â†’ ì „ì´ ì„±ëŠ¥ í™•ì¸                                   |
| ì‚¬ì „í•™ìŠµ êµ¬ë¶„ | ì§€ë„ ì‚¬ì „í•™ìŠµ(supervised pre-training) vs ë¹„ì§€ë„ ì‚¬ì „í•™ìŠµ(unsupervised pre-training) ëŒ€ë¹„ |
</aside>

---

---

## ğŸ”¬ ì‹¤í—˜ê³¼ì •

### ğŸ“šÂ ê´€ë ¨ ì—°êµ¬

### ë²ˆì—­

ì›ë¬¸ (Related Work)

Visualizing features to gain intuition about the network is common practice, but mostly limited to the 1st layer where projections to pixel space are possible. In higher layers this is not the case, and there are limited methods for interpreting activity. (Erhan et al., 2009) find the optimal stimulus for each unit by performing gradient descent in image space to maximize the unitâ€™s activation. This requires a careful initialization and does not give any information about the unitâ€™s invariances. Motivated by the latterâ€™s short-coming, (Le et al., 2010) (extending an idea by (Berkes & Wiskott, 2006)) show how the Hessian of a given unit may be computed numerically around the optimal response, giving some insight into invariances. The problem is that for higher layers, the invariances are extremely complex so are poorly captured by a simple quadratic approximation. Our approach, by contrast, provides a non-parametric view of invariance, showing which patterns from the training set activate the feature map.

(Donahue et al., 2013) show visualizations that identify patches within a dataset that are responsible for strong activations at higher layers in the model. Our visualizations differ in that they are not just crops of input images, but rather top-down projections that reveal structures within each patch that stimulate a particular feature map.

---

ë²ˆì—­ë¬¸ (ê´€ë ¨ ì—°êµ¬)

ë„¤íŠ¸ì›Œí¬ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ìœ„í•´ íŠ¹ì§•(feature)ì„ ì‹œê°í™”í•˜ëŠ” ê²ƒì€ í”í•œ ê´€í–‰ì´ë‹¤. ê·¸ëŸ¬ë‚˜ ëŒ€ë¶€ë¶„ì€ í”½ì…€ ê³µê°„ìœ¼ë¡œ ì§ì ‘ íˆ¬ì˜ì´ ê°€ëŠ¥í•œ **1ì¸µ(1st layer)**ì— êµ­í•œëœë‹¤. ë” ë†’ì€ ì¸µì—ì„œëŠ” ì´ëŸ¬í•œ ì ‘ê·¼ì´ ë¶ˆê°€ëŠ¥í•˜ë©°, í™œë™(activity)ì„ í•´ì„í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ì œí•œì ì´ë‹¤.

(Erhan et al., 2009)ì€ ê° ìœ ë‹›ì˜ í™œì„±í™”ë¥¼ ìµœëŒ€í™”í•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ ê³µê°„ì—ì„œ ê²½ì‚¬í•˜ê°•ë²•ì„ ìˆ˜í–‰í•˜ì—¬, ê° ìœ ë‹›ì— ëŒ€í•œ ìµœì  ìê·¹(optimal stimulus)ì„ ì°¾ì•˜ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŠ” ì´ˆê¸°í™”ì— ë¯¼ê°í•˜ë©°, ìœ ë‹›ì˜ ë¶ˆë³€ì„±(invariances)ì— ê´€í•œ ì •ë³´ëŠ” ì œê³µí•˜ì§€ ëª»í•œë‹¤. ì´ëŸ¬í•œ í•œê³„ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´, (Le et al., 2010)ì€ (Berkes & Wiskott, 2006)ì˜ ì•„ì´ë””ì–´ë¥¼ í™•ì¥í•˜ì—¬, ì£¼ì–´ì§„ ìœ ë‹›ì˜ ìµœì  ë°˜ì‘ ê·¼ì²˜ì—ì„œ Hessianì„ ìˆ˜ì¹˜ì ìœ¼ë¡œ ê³„ì‚°í•¨ìœ¼ë¡œì¨ ë¶ˆë³€ì„±ì— ëŒ€í•œ ì¼ë¶€ í†µì°°ì„ ì œê³µí–ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë†’ì€ ê³„ì¸µì—ì„œëŠ” ë¶ˆë³€ì„±ì´ ë§¤ìš° ë³µì¡í•˜ê¸° ë•Œë¬¸ì— ë‹¨ìˆœí•œ ì´ì°¨ ê·¼ì‚¬(quadratic approximation)ë¡œëŠ” ì˜ í¬ì°©ë˜ì§€ ì•ŠëŠ”ë‹¤. ë°˜ë©´, ìš°ë¦¬ì˜ ì ‘ê·¼ë²•ì€ í›ˆë ¨ ë°ì´í„°ì—ì„œ ì–´ë–¤ íŒ¨í„´ì´ feature mapì„ í™œì„±í™”í•˜ëŠ”ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” **ë¹„ëª¨ìˆ˜ì (non-parametric) ê´€ì ì˜ ë¶ˆë³€ì„± ì‹œê°í™”**ë¥¼ ì œê³µí•œë‹¤.

(Donahue et al., 2013)ì€ ë°ì´í„°ì…‹ ë‚´ì—ì„œ ë†’ì€ ê³„ì¸µ feature mapì„ ê°•í•˜ê²Œ í™œì„±í™”ì‹œí‚¤ëŠ” ì´ë¯¸ì§€ íŒ¨ì¹˜ë¥¼ ì‹ë³„í•˜ëŠ” ì‹œê°í™”ë¥¼ ì œì‹œí–ˆë‹¤. ìš°ë¦¬ì˜ ì‹œê°í™”ëŠ” ë‹¨ìˆœíˆ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì˜ë¼ë‚´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **top-down projection(ìƒí–¥ì‹ íˆ¬ì˜)**ì„ í†µí•´ íŠ¹ì • feature mapì„ ìê·¹í•˜ëŠ” íŒ¨ì¹˜ ë‚´ë¶€ì˜ êµ¬ì¡°ë¥¼ ë“œëŸ¬ë‚¸ë‹¤ëŠ” ì ì—ì„œ ë‹¤ë¥´ë‹¤.

<aside>

## 1.1. ê´€ë ¨ ì—°êµ¬

- ëŒ€ë¶€ë¶„ì€ ì²«ë²ˆì§¸ ë ˆì´ì–´ë§Œ ì§ì ‘ ì‹œê°í™”í•œë‹¤. ë” ê¹Šì€ ì¸µì—ì„œëŠ” ì´ëŸ¬í•œ ì ‘ê·¼ì´ ì œí•œì ì´ë‹¤.
- ê° ë‰´ëŸ° ìœ ë‹›ì˜ í™œì„±í™”ë¥¼ ìµœëŒ€í™”í•˜ê¸° ìœ„í•´ì„œ, ì´ë¯¸ì§€ ê³µê°„ì—ì„œ ê° ìœ ë‹›ì˜ optimal stimulus(ìµœì  ìê·¹)ì„ ì°¾ì•˜ìœ¼ë‚˜ ì´ëŠ” ì´ˆê¸°í™”ì— ë¯¼ê°í•˜ì—¬ ìœ ë‹›ì˜ invariances(ë¶ˆë³€ì„±)ì— ëŒ€í•œ ì •ë³´ëŠ” ì œê³µí•˜ì§€ ëª»í•œë‹¤. â†’ ì´ëŸ¬í•œ ë‹¨ì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ Hessianì„ ìˆ˜ì¹˜ì ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ ì¼ë¶€ í†µì°°ì„ ì œê³µí–ˆìœ¼ë‚˜ ê¹Šì–´ì§ˆìˆ˜ë¡ í†µì°°ì„ ì œê³µí•˜ì§€ ì•ŠëŠ”ë‹¤(ì´ì°¨ ê·¼ì‚¬ì˜ ë‹¨ì )
- ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë¹„ëª¨ìˆ˜ì  ê´€ì ì˜ ë¶ˆë³€ì„± ì‹œê°í™”ë¥¼ ì œê³µ, ì´ë¯¸ì§€ë¥¼ ì˜ë¼ë‚´ëŠ”ê²Œ ì•„ë‹ˆë¼ top-down projectionì„ í†µí•´ íŠ¹ì • í”¼ì³ë§µì„ ìê·¹í•˜ëŠ” íŒ¨ì¹˜ ë‚´ë¶€ì˜ ë“œëŸ¬ë‚¸ë‹¤

<aside>

**í—¤ì„¸ í–‰ë ¬(Hessian Matrix)**

- ì–´ë–¤ í•¨ìˆ˜ f(x)ì˜ **Hessian í–‰ë ¬(Hessian matrix)**ì€ **ì´ì°¨ ë„í•¨ìˆ˜(ì´ê³„ ë¯¸ë¶„)**ë¥¼ ëª¨ì•„ë†“ì€ **ì •ë°©í–‰ë ¬**ì…ë‹ˆë‹¤.
- ì˜ˆë¥¼ ë“¤ì–´, f(xâ‚, xâ‚‚, â€¦, xâ‚™)ì´ nì°¨ì› ë³€ìˆ˜ xë¥¼ ê°€ì§„ ìŠ¤ì¹¼ë¼ í•¨ìˆ˜ë¼ë©´, Hessian HëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤.

H_ij = âˆ‚Â²f / âˆ‚xáµ¢ âˆ‚xâ±¼

ì¦‰,

H =

[ âˆ‚Â²f/âˆ‚xâ‚Â²   âˆ‚Â²f/âˆ‚xâ‚âˆ‚xâ‚‚  ... ]

[ âˆ‚Â²f/âˆ‚xâ‚‚âˆ‚xâ‚ âˆ‚Â²f/âˆ‚xâ‚‚Â²   ... ]

[   ...           ...     ... ]

- Hessianì€ í•¨ìˆ˜ì˜ **ê³¡ë¥ (curvature)**ì„ ë‚˜íƒ€ë‚´ë©°, í•¨ìˆ˜ê°€ íŠ¹ì • ì§€ì ì—ì„œ ë³¼ë¡(convex)í•œì§€, ì˜¤ëª©(concave)í•œì§€, ë˜ëŠ” ì•ˆì¥ì (saddle point)ì¸ì§€ë¥¼ íŒë³„í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
- **ìµœì í™”(Optimization)**: ê²½ì‚¬í•˜ê°•ë²•(gradient descent)ì€ 1ì°¨ ë¯¸ë¶„(gradient)ë§Œ ì“°ì§€ë§Œ, 2ì°¨ ìµœì í™” ê¸°ë²•(Newton's method ë“±)ì€ Hessianì„ ì´ìš©í•´ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•©ë‹ˆë‹¤.
- **ë¯¼ê°ë„ ë¶„ì„(Sensitivity Analysis)**: íŠ¹ì • ì…ë ¥ ë³€í™”ê°€ ì¶œë ¥ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ëŠ”ì§€, ì¦‰ ëª¨ë¸ì˜ **êµ­ì†Œì  ë¶ˆë³€ì„±(local invariance)**ì„ ë¶„ì„í•  ë•Œ Hessianì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ë‰´ëŸ°ì˜ ì¶œë ¥ì´ ì…ë ¥ ë³€í™”ì— ë”°ë¼ ì–¼ë§ˆë‚˜ ë¯¼ê°í•˜ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€(ê³¡ë¥ )ë¥¼ ë¶„ì„
- ê³¡ë¥ ì´ ë‚®ì€ ë°©í–¥ â†’ ë‰´ëŸ°ì´ ê·¸ ë°©í–¥ì˜ ì…ë ¥ ë³€í™”ì—ëŠ” **ë¶ˆë³€(invariant)**
- ê³¡ë¥ ì´ ë†’ì€ ë°©í–¥ â†’ ë¯¼ê°í•˜ê²Œ ë°˜ì‘ â†’ ì¤‘ìš”í•œ íŒ¨í„´ ë°©í–¥
</aside>

### í•µì‹¬

- ì‹œê°í™” ì—°êµ¬ëŠ” ì´ˆê¸°ì— ì‹œì‘ì¸µì´ë‚˜ ì´ˆê¸°ì¸µì—ë§Œ êµ­í•œë˜ì–´ ê¹Šì€ ì¸µì€ í•´ì„ ë¶ˆê°€ëŠ¥
    - **ìµœì  ìê·¹ íƒìƒ‰ (Erhan et al., 2009)**: ì´ë¯¸ì§€ ê³µê°„ì—ì„œ ê²½ì‚¬í•˜ê°•ë²• â†’ í™œì„±í™” ê·¹ëŒ€í™”
        
        â†’ ë‹¨ì : ì´ˆê¸°í™” ë¯¼ê°, ë¶ˆë³€ì„± ì •ë³´ ì—†ìŒ
        
    - **Hessian ê¸°ë°˜ ë¶ˆë³€ì„± ë¶„ì„ (Le et al., 2010)**: Hessian ê·¼ì‚¬ë¡œ ë¶ˆë³€ì„± íŒŒì•…
        
        â†’ ë‹¨ì : ê³ ì°¨ì› ì¸µì˜ ë³µì¡í•œ ë¶ˆë³€ì„±ì„ ë‹¨ìˆœ ì´ì°¨ì‹ìœ¼ë¡œ ì„¤ëª… ë¶ˆê°€
        
    - **íŒ¨ì¹˜ ê¸°ë°˜ ì‹œê°í™” (Donahue et al., 2013)**: ë°ì´í„°ì…‹ì—ì„œ ê°•í•œ í™œì„±í™”ë¥¼ ì¼ìœ¼í‚¤ëŠ” íŒ¨ì¹˜ ì‹ë³„
        
        â†’ ë‹¨ì : ë‹¨ìˆœ crop, feature map ë‚´ë¶€ êµ¬ì¡°ëŠ” ì„¤ëª…í•˜ì§€ ëª»í•¨
        

### âœ… 5ì¤„ ìš”ì•½

- ê³¼ê±° ì‹œê°í™” ì—°êµ¬ëŠ” ì£¼ë¡œ ì²« ë²ˆì§¸ ì¸µì— ì§‘ì¤‘
- Erhan et al. (2009): ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ìµœì  ìê·¹ íƒìƒ‰, ë¶ˆë³€ì„± ì„¤ëª… ë¶€ì¡±
- Le et al. (2010): Hessian ê·¼ì‚¬ë¡œ ë¶ˆë³€ì„± ë¶„ì„, ê³ ì¸µì—ì„œëŠ” ë¶€ì •í™•
- Donahue et al. (2013): ë°ì´í„°ì…‹ íŒ¨ì¹˜ ê¸°ë°˜ ì‹œê°í™”, êµ¬ì¡°ì  í•´ì„ ì œí•œ
- ë³¸ ë…¼ë¬¸: Deconvnetì„ í†µí•´ **ë¹„ëª¨ìˆ˜ì , êµ¬ì¡°ì  ì‹œê°í™”** ì œê³µ â†’ ê³ ì¸µ feature í•´ì„ ê°€ëŠ¥

### ğŸ“ŒÂ ì •ë¦¬

| ì—°êµ¬                  | ë°©ë²•                               | í•œê³„                          |
| --------------------- | ---------------------------------- | ----------------------------- |
| Erhan et al. (2009)   | ì´ë¯¸ì§€ ê³µê°„ ê²½ì‚¬í•˜ê°• â†’ ìµœì  ìê·¹   | ì´ˆê¸°í™” ë¯¼ê°, ë¶ˆë³€ì„± ì„¤ëª… ë¶ˆê°€ |
| Le et al. (2010)      | Hessian ê·¼ì‚¬ â†’ ë¶ˆë³€ì„± ë¶„ì„         | ê³ ì°¨ì› ì¸µì˜ ë³µì¡ì„± ë°˜ì˜ ëª»í•¨  |
| Donahue et al. (2013) | íŒ¨ì¹˜ ì‹ë³„ â†’ í™œì„±í™” í•´ì„            | ë‹¨ìˆœ crop, êµ¬ì¡° ì„¤ëª… í•œê³„     |
| ë³¸ ë…¼ë¬¸               | Deconvnet ê¸°ë°˜ top-down projection | ê³ ì¸µ feature êµ¬ì¡°ì  í•´ì„ ê°€ëŠ¥ |
</aside>

### ğŸ“šÂ 2. Approach

### ë²ˆì—­

1. ì›ë¬¸ (Approach)
    
    We use standard fully supervised convnet models throughout the paper, as defined by (LeCun et al., 1989) and (Krizhevsky et al., 2012). These models map a color 2D input image xi, via a series of layers, to a probability vector yÌ‚i over the C different classes. Each layer consists of (i) convolution of the previous layer output (or, in the case of the 1st layer, the input image) with a set of learned filters; (ii) passing the responses through a rectified linear function (relu(x) = max(x, 0)); (iii) [optionally] max pooling over local neighborhoods and (iv) [optionally] a local contrast operation that normalizes the responses across feature maps. For more details of these operations, see (Krizhevsky et al., 2012) and (Jarrett et al., 2009). The top few layers of the network are conventional fully-connected networks and the final layer is a softmax classifier. Fig. 3 shows the model used in many of our experiments.
    
    We train these models using a large set of N labeled images {x, y}, where label yi is a discrete variable indicating the true class. A cross-entropy loss function, suitable for image classification, is used to compare yÌ‚i and yi. The parameters of the network (filters in the convolutional layers, weight matrices in the fully-connected layers and biases) are trained by back-propagating the derivative of the loss with respect to the parameters throughout the network, and updating the parameters via stochastic gradient descent. Full details of training are given in Section 3.
    

---

1. ë²ˆì—­ë¬¸ (ì ‘ê·¼ ë°©ë²•)
    
    ìš°ë¦¬ëŠ” ë³¸ ë…¼ë¬¸ ì „ë°˜ì—ì„œ (LeCun et al., 1989) ë° (Krizhevsky et al., 2012)ì—ì„œ ì •ì˜ëœ í‘œì¤€ì ì¸ **ì™„ì „ ì§€ë„ í•™ìŠµ(fully supervised) ConvNet ëª¨ë¸**ì„ ì‚¬ìš©í•œë‹¤. ì´ ëª¨ë¸ì€ 2ì°¨ì› ì»¬ëŸ¬ ì…ë ¥ ì´ë¯¸ì§€ xië¥¼ ì¼ë ¨ì˜ ê³„ì¸µ(layer)ì„ í†µí•´ Cê°œì˜ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ë²¡í„° yÌ‚ië¡œ ë§¤í•‘í•œë‹¤.
    
    ê° ê³„ì¸µì€ ë‹¤ìŒ ì—°ì‚°ìœ¼ë¡œ êµ¬ì„±ëœë‹¤:
    
    (i) ì´ì „ ê³„ì¸µ ì¶œë ¥(í˜¹ì€ 1ì¸µì˜ ê²½ìš° ì…ë ¥ ì´ë¯¸ì§€)ì„ í•™ìŠµëœ í•„í„° ì§‘í•©ìœ¼ë¡œ **í•©ì„±ê³±(convolution)**
    
    (ii) ì¶œë ¥ ê°’ì„ **ReLU(Rectified Linear Unit, relu(x) = max(x, 0))** ë¹„ì„ í˜• í•¨ìˆ˜ì— í†µê³¼
    
    (iii) [ì˜µì…˜] ì§€ì—­ì  ì˜ì—­ì— ëŒ€í•œ **ìµœëŒ€ í’€ë§(max pooling)**
    
    (iv) [ì˜µì…˜] feature map ì „ë°˜ì„ ì •ê·œí™”í•˜ëŠ” **local contrast normalization**
    
    ë„¤íŠ¸ì›Œí¬ì˜ ìƒìœ„ ëª‡ ê°œ ê³„ì¸µì€ ì „í†µì ì¸ **ì™„ì „ì—°ê²°ì¸µ(fully-connected network)**ìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ë§ˆì§€ë§‰ ê³„ì¸µì€ **softmax ë¶„ë¥˜ê¸°**ì´ë‹¤. ê·¸ë¦¼ 3ì€ ë³¸ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ëœ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë³´ì—¬ì¤€ë‹¤.
    
    ëª¨ë¸ í•™ìŠµì—ëŠ” Nê°œì˜ ë¼ë²¨ì´ ë‹¬ë¦° ì´ë¯¸ì§€ {x, y}ê°€ ì‚¬ìš©ëœë‹¤. ì—¬ê¸°ì„œ yiëŠ” ì°¸ í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ ë‚˜íƒ€ë‚´ëŠ” ì´ì‚° ë³€ìˆ˜ì´ë‹¤. ì¶œë ¥ yÌ‚iì™€ yië¥¼ ë¹„êµí•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ ë¶„ë¥˜ì— ì í•©í•œ **êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤(cross-entropy loss)**ì„ ì‚¬ìš©í•œë‹¤. ë„¤íŠ¸ì›Œí¬ì˜ íŒŒë¼ë¯¸í„°(í•©ì„±ê³± ê³„ì¸µì˜ í•„í„°, ì™„ì „ì—°ê²°ì¸µì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬, í¸í–¥)ëŠ” ì†ì‹¤ì˜ ë„í•¨ìˆ˜ë¥¼ ë„¤íŠ¸ì›Œí¬ ì „ì²´ë¡œ **ì—­ì „íŒŒ(backpropagation)** ì‹œì¼œ êµ¬í•˜ê³ , ì´ë¥¼ ì´ìš©í•´ **í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(SGD)**ìœ¼ë¡œ ê°±ì‹ í•œë‹¤. í•™ìŠµì— ëŒ€í•œ ì„¸ë¶€ ì‚¬í•­ì€ Section 3ì—ì„œ ê¸°ìˆ í•œë‹¤.
    
2. ì›ë¬¸ (2.1 Visualization with a Deconvnet)
    
    Understanding the operation of a convnet requires interpreting the feature activity in intermediate layers. We present a novel way to map these activities back to the input pixel space, showing what input pattern originally caused a given activation in the feature maps. We perform this mapping with a Deconvolutional Network (deconvnet) (Zeiler et al., 2011). A deconvnet can be thought of as a convnet model that uses the same components (filtering, pooling) but in reverse, so instead of mapping pixels to features does the opposite. In (Zeiler et al., 2011), deconvnets were proposed as a way of performing unsupervised learning. Here, they are not used in any learning capacity, just as a probe of an already trained convnet.
    
    To examine a convnet, a deconvnet is attached to each of its layers, as illustrated in Fig. 1(top), providing a continuous path back to image pixels. To start, an input image is presented to the convnet and features computed throughout the layers. To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer. Then we successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity in the layer beneath that gave rise to the chosen activation. This is then repeated until input pixel space is reached.
    
    Unpooling: In the convnet, the max pooling operation is non-invertible, however we can obtain an approximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables. In the deconvnet, the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. See Fig. 1(bottom) for an illustration of the procedure.
    
    Rectification: The convnet uses relu non-linearities, which rectify the feature maps thus ensuring the feature maps are always positive. To obtain valid feature reconstructions at each layer (which also should be positive), we pass the reconstructed signal through a relu non-linearity.
    
    Filtering: The convnet uses learned filters to convolve the feature maps from the previous layer. To invert this, the deconvnet uses transposed versions of the same filters, but applied to the rectified maps, not the output of the layer beneath. In practice this means flipping each filter vertically and horizontally.
    
    Projecting down from higher layers uses the switch settings generated by the max pooling in the convnet on the way up. As these switch settings are peculiar to a given input image, the reconstruction obtained from a single activation thus resembles a small piece of the original input image, with structures weighted according to their contribution toward to the feature activation. Since the model is trained discriminatively, they implicitly show which parts of the input image are discriminative. Note that these projections are not samples from the model, since there is no generative process involved.
    

---

1. ë²ˆì—­ë¬¸ (2.1 Deconvnetì„ ì´ìš©í•œ ì‹œê°í™”)
    
    ConvNetì˜ ë™ì‘ì„ ì´í•´í•˜ë ¤ë©´ ì¤‘ê°„ ê³„ì¸µì˜ íŠ¹ì§• í™œì„±(feature activity)ì„ í•´ì„í•´ì•¼ í•œë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ í™œë™ì„ ë‹¤ì‹œ ì…ë ¥ í”½ì…€ ê³µê°„ìœ¼ë¡œ ë§¤í•‘í•˜ì—¬, ì–´ë–¤ ì…ë ¥ íŒ¨í„´ì´ íŠ¹ì • feature mapì„ í™œì„±í™”ì‹œì¼°ëŠ”ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì‹œí•œë‹¤. ì´ ë§¤í•‘ì€ **Deconvolutional Network (deconvnet)** (Zeiler et al., 2011)ì„ ì´ìš©í•´ ìˆ˜í–‰ëœë‹¤. Deconvnetì€ ConvNetê³¼ ë™ì¼í•œ êµ¬ì„± ìš”ì†Œ(í•„í„°ë§, í’€ë§)ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ë°©í–¥ì´ ë°˜ëŒ€ë¼ì„œ **í”½ì…€â†’íŠ¹ì§•**ì´ ì•„ë‹Œ **íŠ¹ì§•â†’í”½ì…€**ë¡œ ì—­íˆ¬ì˜í•œë‹¤. (Zeiler et al., 2011)ì—ì„œëŠ” deconvnetì´ ë¹„ì§€ë„ í•™ìŠµìš©ìœ¼ë¡œ ì œì•ˆë˜ì—ˆìœ¼ë‚˜, ì—¬ê¸°ì„œëŠ” í•™ìŠµì´ ì•„ë‹Œ **ì´ë¯¸ í•™ìŠµëœ ConvNetì„ ë¶„ì„(probe)**í•˜ëŠ” ìš©ë„ë¡œ ì‚¬ìš©ëœë‹¤.
    
    ë¶„ì„ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤: convnetì˜ ê° ê³„ì¸µì— deconvnetì„ ì—°ê²°í•˜ë©´(Fig.1 ìƒë‹¨), ì…ë ¥ ì´ë¯¸ì§€ë¥¼ convnetì— ë„£ì–´ featureë¥¼ ê³„ì‚°í•œ ë’¤, íŠ¹ì • activationì„ ì„ íƒí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ 0ìœ¼ë¡œ ë§Œë“ ë‹¤. ì´ feature mapì„ í•´ë‹¹ ê³„ì¸µì˜ deconvnetì— ì…ë ¥ìœ¼ë¡œ ë„£ì–´ (i) unpool, (ii) rectify, (iii) filter ê³¼ì •ì„ ê±°ì³ ë°”ë¡œ ì•„ë˜ ê³„ì¸µì˜ activityë¥¼ ë³µì›í•œë‹¤. ì´ ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ ìµœì¢…ì ìœ¼ë¡œ ì…ë ¥ í”½ì…€ ê³µê°„ì— ë„ë‹¬í•œë‹¤.
    
    - **Unpooling**: ConvNetì˜ max poolingì€ ë¹„ê°€ì—­ì ì´ë‹¤. ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´, pooling ì‹œ ê° ì˜ì—­ì˜ ìµœëŒ“ê°’ ìœ„ì¹˜ë¥¼ switch ë³€ìˆ˜ë¡œ ê¸°ë¡í•œë‹¤. Deconvnetì—ì„œëŠ” ì´ switchë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒìœ„ ê³„ì¸µì˜ ë³µì› ê²°ê³¼ë¥¼ ì •í™•í•œ ìœ„ì¹˜ì— ë°°ì¹˜í•œë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ìê·¹ êµ¬ì¡°(stimulus structure)ê°€ ë³´ì¡´ëœë‹¤.
    - **Rectification**: ConvNetì€ ReLU ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ feature mapì„ í•­ìƒ ì–‘ìˆ˜ë¡œ ë§Œë“ ë‹¤. Deconvnetì—ì„œë„ ë§ˆì°¬ê°€ì§€ë¡œ ReLUë¥¼ ì ìš©í•´ ë³µì›ëœ featureê°€ ì–‘ìˆ˜ë¡œ ìœ ì§€ë˜ë„ë¡ í•œë‹¤.
    - **Filtering**: ConvNetì€ í•™ìŠµëœ í•„í„°ë¡œ feature mapì„ í•©ì„±ê³±í•œë‹¤. ì´ë¥¼ ì—­ì „í•˜ê¸° ìœ„í•´ deconvnetì€ **ê°™ì€ í•„í„°ì˜ ì „ì¹˜(transposed) ë²„ì „**ì„ ì‚¬ìš©í•œë‹¤(ì¦‰, í•„í„°ë¥¼ ìˆ˜ì§Â·ìˆ˜í‰ìœ¼ë¡œ ë’¤ì§‘ëŠ”ë‹¤).
    
    ìƒìœ„ ê³„ì¸µì—ì„œ í•˜ìœ„ ê³„ì¸µìœ¼ë¡œì˜ íˆ¬ì˜ ì‹œ, ConvNet í•™ìŠµ ê³¼ì •ì—ì„œ ê¸°ë¡ëœ pooling switchê°€ ì‚¬ìš©ëœë‹¤. ì´ switchëŠ” íŠ¹ì • ì…ë ¥ ì´ë¯¸ì§€ì— ì¢…ì†ì ì´ë¯€ë¡œ, í•œ ê°œì˜ activationìœ¼ë¡œë¶€í„° ì–»ì€ ë³µì› ê²°ê³¼ëŠ” ì›ë˜ ì…ë ¥ ì´ë¯¸ì§€ì˜ ì¼ë¶€ë¶„ê³¼ ìœ ì‚¬í•˜ë‹¤. ì´ëŠ” featureê°€ ì…ë ¥ ì´ë¯¸ì§€ì˜ ì–´ë–¤ êµ¬ì¡°ì— ì˜í•´ ìê·¹ë˜ëŠ”ì§€ë¥¼ ê°€ì¤‘ì¹˜ í˜•íƒœë¡œ ë³´ì—¬ì¤€ë‹¤. ëª¨ë¸ì€ íŒë³„ì (discriminative)ìœ¼ë¡œ í•™ìŠµë˜ë¯€ë¡œ, ì´ ì‹œê°í™”ëŠ” ê²°êµ­ ì…ë ¥ ì´ë¯¸ì§€ì—ì„œ **ì–´ë–¤ ë¶€ë¶„ì´ íŒë³„ì— ì¤‘ìš”í–ˆëŠ”ì§€**ë¥¼ ë“œëŸ¬ë‚¸ë‹¤. ì£¼ì˜í•  ì ì€, ì´ëŸ¬í•œ projectionì€ **ìƒì„± ëª¨ë¸ì˜ ìƒ˜í”Œì´ ì•„ë‹ˆë¼ ë‹¨ìˆœíˆ ì—­íˆ¬ì˜ëœ êµ¬ì¡°**ë¼ëŠ” ê²ƒì´ë‹¤.
    

<aside>

# 2. Approach

- **ì§€ë„í•™ìŠµ**
- **Layerêµ¬ì¡° : Conv â†’ ReLU â†’ (ì˜µì…˜) Max Pooling : Localì— ëŒ€í•´ â†’ (ì˜µì…˜) Local Contrast Normalization : feature map ì „ë°˜ì„ ì •ê·œí™”**
- ë„¤íŠ¸ì›Œí¬ê°€ ê¹Šì–´ì§€ë©´ ì†Œìˆ˜ì˜ fc layerë¡œ êµ¬ì„±, ë§ˆì§€ë§‰ì€ **softmax clf.**
- **ì•„í‚¤í…ì³**

![image.png]((ZFNet)%20Visualizing%20and%20Understanding%20Convolutiona%2025ca9b246de18068af2bc450e4cf0526/image.png)

- ì†ì‹¤í•¨ìˆ˜ : **Cross-entropy**
- Optimizer : **SGD(mini-batch)**

### âœ… 5ì¤„ ìš”ì•½

- ConvNetì€ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ê³„ì¸µì  ë³€í™˜ì„ í†µí•´ í´ë˜ìŠ¤ í™•ë¥ ë¡œ ë§¤í•‘
- ê³„ì¸µì€ í•©ì„±ê³±, ReLU, í’€ë§, ì •ê·œí™”ë¡œ êµ¬ì„±
- ìƒìœ„ ê³„ì¸µì€ fully-connected + softmax ë¶„ë¥˜ê¸°
- êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ê³¼ backpropagationìœ¼ë¡œ í•™ìŠµ
- í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(SGD)ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ìµœì í™”

### ğŸ“ŒÂ ì •ë¦¬

| ìš”ì†Œ          | ì„¤ëª…                              |
| ------------- | --------------------------------- |
| ì…ë ¥          | ì»¬ëŸ¬ 2D ì´ë¯¸ì§€ (xi)               |
| ì¶œë ¥          | í´ë˜ìŠ¤ í™•ë¥  ë²¡í„° (Å·i)             |
| ê³„ì¸µ êµ¬ì„±     | í•©ì„±ê³± + ReLU + (í’€ë§) + (ì •ê·œí™”) |
| ìƒìœ„ êµ¬ì¡°     | Fully-connected layers            |
| ìµœì¢… ë¶„ë¥˜ê¸°   | Softmax                           |
| ì†ì‹¤ í•¨ìˆ˜     | Cross-entropy                     |
| í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ | Backpropagation + SGD             |

## 2.1 Visualization with a Deconvnet

- **Deconvolutional Network : Convì™€ ë™ì¼í•œ êµ¬ì„±ì´ì§€ë§Œ í”½ì…€ â†’ íŠ¹ì§• íˆ¬ì˜ê³¼ ë°˜ëŒ€ë¡œ íŠ¹ì§• â†’ í”½ì…€ íˆ¬ì˜ì„í•œë‹¤.**
    - ê¸°ì¡´ì—ëŠ” ë¹„ì§€ë„í•™ìŠµìš©ìœ¼ë¡œ ì œì•ˆë˜ì—ˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” **ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì„ probe**í•˜ëŠ” ìš©ë„ë¡œ ì‚¬ìš©ëœë‹¤.
- ë¶„ì„ê³¼ì •
    1. **convnetì— deconvnetì„ ì—°ê²°í•œë‹¤.**
    2. **ì´ë¯¸ì§€ë¥¼ convnetì— ë„£ì€ í›„ featureì„ ê³„ì‚°í•˜ê³ , íŠ¹ì • í™œì„±ì„ ì„ íƒí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ 0ìœ¼ë¡œ ë§Œë“ ë‹¤.**
    3. **ì´ featureì„ deconvnetì— ë„£ì–´ì„œ ë³µì›í•œë‹¤.**
        1. **unpool**
        2. **rectify**
        3. **filter ê³¼ì •ì„ ê±°ì³ ë°”ë¡œ ì•„ë˜ ê³„ì¸µì˜ activityë¥¼ ë³µì›**
- í•µì‹¬ì—°ì‚°
    - **Unpooling** : max poolingì€ ë¹„ê°€ì—­ì ì¸ë°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **poolingì‹œ ê° ì˜ì—­ì˜ ìµœëŒ€ê°’ì˜ ìœ„ì¹˜ë¥¼ switchë³€ìˆ˜ë¡œ ê¸°ë¡**í•˜ê³ , ê·¸ ê°’ì„ ë°”íƒ•ìœ¼ë¡œ ë³µì› ê²°ê³¼ë¥¼ ì •í™•í•œ ìœ„ì¹˜ì— ë°°ì¹˜
    - **Rectification** : ConvNetì²˜ëŸ¼ **ReLU**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³µì›(**ë³µì›ëœ featureê°€ ì–‘ìˆ˜**)
    - **Filtering** : **ConvNetì˜ í•™ìŠµëœ í•„í„°ë¥¼ ì „ì¹˜ ë²„ì „**ì„ ì‚¬ìš©í•œë‹¤.(ìˆ˜ì§, ìˆ˜í‰ë°©í–¥)
- í•œê°œì˜ í™œì„±ìœ¼ë¡œë¶€í„° ì–»ëŠ” ê²°ê³¼ëŠ” ì´ë¯¸ì§€ì˜ ì–´ë–¤ êµ¬ì¡°ì˜ ì¼ë¶€ë¶„ê³¼ ìœ ì‚¬í•˜ë‹¤. **ëª¨ë¸ì€ íŒë³„ì ìœ¼ë¡œ í•™ìŠµë˜ë¯€ë¡œ, ì…ë ¥ì´ë¯¸ì§€ì—ì„œ ì–´ë–¤ë¶€ë¶„ì´ ì¤‘ìš”í–ˆëŠ”ì§€ë¥¼ ë“œëŸ¬**ë‚¸ë‹¤. ë‹¤ë§Œ **ìƒì„±ëª¨ë¸ì˜ ìƒ˜í”Œì´ ì•„ë‹ˆë¼, ë‹¨ìˆœíˆ ì—­íˆ¬ì˜**ëœë‹¤ëŠ” ì ì´ë‹¤. â†’ **ìƒì„±ëª¨ë¸ì´ ì•„ë‹ˆë¼ ì—­íˆ¬ì˜**ì´ë‹¤. â†’ ì¦‰ ëª¨ë¸ì´ ì…ë ¥êµ¬ì¡°ì˜ ì–´ë–¤ë¶€ë¶„ì„ ê°€ì§€ê³  íŒë‹¨í–ˆëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ ë³´ì—¬ì¤€ë‹¤.
- ë¶„ì„ êµ¬ì¡°
    
    ![image.png]((ZFNet)%20Visualizing%20and%20Understanding%20Convolutiona%2025ca9b246de18068af2bc450e4cf0526/image%201.png)
    

### âœ… 5ì¤„ ìš”ì•½

- Deconvnetì„ ì´ìš©í•´ ì¤‘ê°„ì¸µ feature mapì„ ì…ë ¥ í”½ì…€ ê³µê°„ìœ¼ë¡œ ë³µì›
- ê³¼ì •: (Unpool â†’ ReLU â†’ Filter) ë°˜ë³µ
- pooling switchë¡œ ì›ë˜ ìê·¹ ìœ„ì¹˜ ë³µì›
- ê²°ê³¼: íŠ¹ì • activationì´ ì…ë ¥ ì´ë¯¸ì§€ì˜ ì–´ë–¤ ë¶€ë¶„ì— ì˜í•´ ìœ ë°œë˜ì—ˆëŠ”ì§€ í™•ì¸ ê°€ëŠ¥
- ì´ëŠ” ìƒì„±ì´ ì•„ë‹Œ íŒë³„ ê¸°ë°˜ projection â†’ ConvNetì˜ íŒë³„ ê·¼ê±°ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì‹œê°í™”

### ğŸ“ŒÂ ì •ë¦¬

| ë‹¨ê³„      | ConvNet (ì •ë°©í–¥) | Deconvnet (ì—­ë°©í–¥)       |
| --------- | ---------------- | ------------------------ |
| Pooling   | Max pooling      | Unpooling (switch ì‚¬ìš©)  |
| ReLU      | ReLU             | ReLU                     |
| Filtering | Learned filter   | Transposed filter (flip) |
| ê²°ê³¼      | Feature map      | ì…ë ¥ ê³µê°„ ë³µì›           |
</aside>

### ğŸ“šÂ 3. Training Detail

### ë²ˆì—­

ì›ë¬¸ (3. Training Details)

We now describe the large convnet model that will be visualized in Section 4. The architecture, shown in Fig. 3, is similar to that used by (Krizhevsky et al., 2012) for ImageNet classification. One difference is that the sparse connections used in Krizhevskyâ€™s layers 3,4,5 (due to the model being split across 2 GPUs) are replaced with dense connections in our model.

Other important differences relating to layers 1 and 2 were made following inspection of the visualizations in Fig. 6, as described in Section 4.1.

The model was trained on the ImageNet 2012 training set (1.3 million images, spread over 1000 different classes). Each RGB image was preprocessed by resizing the smallest dimension to 256, cropping the center 256x256 region, subtracting the per-pixel mean (across all images) and then using 10 different sub-crops of size 224x224 (corners + center with(out) horizontal flips). Stochastic gradient descent with a mini-batch size of 128 was used to update the parameters, starting with a learning rate of 10âˆ’2, in conjunction with a momentum term of 0.9. We anneal the learning rate throughout training manually when the validation error plateaus. Dropout (Hinton et al., 2012) is used in the fully connected layers (6 and 7) with a rate of 0.5. All weights are initialized to 10âˆ’2 and biases are set to 0.

Visualization of the first layer filters during training reveals that a few of them dominate, as shown in Fig. 6(a). To combat this, we renormalize each filter in the convolutional layers whose RMS value exceeds a fixed radius of 10âˆ’1 to this fixed radius. This is crucial, especially in the first layer of the model, where the input images are roughly in the [-128,128] range. As in (Krizhevsky et al., 2012), we produce multiple different crops and flips of each training example to boost training set size. We stopped training after 70 epochs, which took around 12 days on a single GTX580 GPU, using an implementation based on (Krizhevsky et al., 2012).

---

ë²ˆì—­ë¬¸ (í›ˆë ¨ ì„¸ë¶€ ì‚¬í•­)

ì´ì œ Section 4ì—ì„œ ì‹œê°í™”ë  ëŒ€ê·œëª¨ ConvNet ëª¨ë¸ì„ ì„¤ëª…í•œë‹¤. ì•„í‚¤í…ì²˜(Fig. 3)ëŠ” ImageNet ë¶„ë¥˜ë¥¼ ìœ„í•´ (Krizhevsky et al., 2012)ì´ ì‚¬ìš©í•œ ê²ƒê³¼ ìœ ì‚¬í•˜ë‹¤. ê·¸ëŸ¬ë‚˜ Krizhevsky ëª¨ë¸ì—ì„œ 2ê°œì˜ GPUë¡œ ë¶„ì‚° í•™ìŠµì„ í–ˆê¸° ë•Œë¬¸ì— 3,4,5ì¸µì€ **í¬ì†Œ ì—°ê²°(sparse connections)**ì„ ì‚¬ìš©í–ˆì§€ë§Œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ë¥¼ **ë°€ì§‘ ì—°ê²°(dense connections)**ë¡œ ëŒ€ì²´í•˜ì˜€ë‹¤.

ë˜í•œ Fig. 6ì˜ ì‹œê°í™” ê²°ê³¼(Section 4.1 ì°¸ê³ )ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 1ì¸µê³¼ 2ì¸µì— ì¤‘ìš”í•œ ìˆ˜ì •ì´ ì´ë£¨ì–´ì¡Œë‹¤.

ëª¨ë¸ì€ ImageNet 2012 í•™ìŠµì…‹(130ë§Œ ì¥, 1000 í´ë˜ìŠ¤)ì—ì„œ í›ˆë ¨ë˜ì—ˆë‹¤. ê° RGB ì´ë¯¸ì§€ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì „ì²˜ë¦¬ë˜ì—ˆë‹¤:

- ê°€ì¥ ì§§ì€ ë³€ì„ 256ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
- ì¤‘ì•™ 256x256 ì˜ì—­ì„ í¬ë¡­
- ì „ì²´ ì´ë¯¸ì§€ì˜ í”½ì…€ë³„ í‰ê· ì„ ë¹¼ì¤Œ (mean subtraction)
- 224x224 í¬ê¸°ì˜ ì„œë¸Œ í¬ë¡­ 10ê°œ(ëª¨ì„œë¦¬ 4ê°œ + ì¤‘ì•™, ì¢Œìš° ë°˜ì „ í¬í•¨) ìƒì„±

ìµœì í™”ëŠ” í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(SGD)ìœ¼ë¡œ ìˆ˜í–‰ë˜ì—ˆìœ¼ë©°, mini-batch í¬ê¸°ëŠ” 128ì´ì—ˆë‹¤. í•™ìŠµë¥ ì€ 10^-2ë¡œ ì‹œì‘í•˜ê³ , ëª¨ë©˜í…€(momentum) 0.9ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤. ê²€ì¦ ì˜¤ë¥˜ê°€ plateauì— ë„ë‹¬í•˜ë©´ í•™ìŠµë¥ ì„ ìˆ˜ë™ìœ¼ë¡œ ê°ì†Œì‹œì¼°ë‹¤. ì™„ì „ ì—°ê²°ì¸µ(6,7ì¸µ)ì—ì„œëŠ” Dropout(Hinton et al., 2012)ì„ 0.5 ë¹„ìœ¨ë¡œ ì ìš©í•˜ì˜€ë‹¤. ëª¨ë“  ê°€ì¤‘ì¹˜ëŠ” 10^-2ë¡œ ì´ˆê¸°í™”í•˜ì˜€ê³ , í¸í–¥ì€ 0ìœ¼ë¡œ ì„¤ì •í•˜ì˜€ë‹¤.

í›ˆë ¨ ì¤‘ 1ì¸µ í•„í„°ë¥¼ ì‹œê°í™”í•œ ê²°ê³¼ ì¼ë¶€ í•„í„°ê°€ ì§€ë‚˜ì¹˜ê²Œ ì§€ë°°ì ì„ì„ í™•ì¸í•˜ì˜€ë‹¤(Fig. 6(a)). ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ RMS ê°’ì´ 10^-1ì„ ì´ˆê³¼í•˜ëŠ” í•„í„°ëŠ” ê°•ì œë¡œ ì¬ì •ê·œí™”(renormalization)í•˜ì—¬ í•´ë‹¹ ë°˜ê²½ìœ¼ë¡œ ë§ì·„ë‹¤. ì´ëŠ” íŠ¹íˆ ì…ë ¥ ì´ë¯¸ì§€ê°€ [-128,128] ë²”ìœ„ì— ì¡´ì¬í•˜ëŠ” 1ì¸µì—ì„œ ì¤‘ìš”í•˜ë‹¤. (Krizhevsky et al., 2012)ì™€ ë§ˆì°¬ê°€ì§€ë¡œ í•™ìŠµ ì˜ˆì œë¥¼ ì¦ê°•í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ í¬ë¡­ê³¼ í”Œë¦½ì„ ì‚¬ìš©í–ˆë‹¤. í•™ìŠµì€ 70 epoch ë™ì•ˆ ìˆ˜í–‰ë˜ì—ˆìœ¼ë©°, GTX580 GPU í•œ ì¥ì—ì„œ ì•½ 12ì¼ì´ ì†Œìš”ë˜ì—ˆë‹¤. êµ¬í˜„ì€ Krizhevsky et al.(2012)ì„ ê¸°ë°˜ìœ¼ë¡œ í–ˆë‹¤.

<aside>

# 3. í•™ìŠµ ì„¸ë¶€ ì‚¬í•­

- AlexNetì˜ GPUë¶„ì‚° í•™ìŠµì„ í–ˆê¸°ë•Œë¬¸ì— 3, 4, 5ì¸µì€ **sparse connections(í¬ì†Œ ì—°ê²° - ìš°ë¦¬ê°€ ê¸°ì¡´ì— ì•„ëŠ” sparseê°€ ì•„ë‹ˆë¼, GPUë¡œ ì¸í•œ ëª¨ë¸ ì•„í‚¤í…ì³ ë¶„ì‚°ì„ ì˜ë¯¸)**ì„ ì‚¬ìš©í–ˆì§€ë§Œ, ë³¸ ëª¨ë¸ì—ì„œëŠ” **dense connections**ë¡œ ëŒ€ì²´
    - ë˜í•œ 1ì¸µê³¼ 2ì¸µì— ì„¸ë¶€ ìˆ˜ì •ì´ ì´ë£¨ì–´ì§(ì‹œê°í™” ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ)
- AlexNetê³¼ ë™ì¼í•˜ê²Œ ì¦ê°•
- 1ì¸µ í•„í„°ë¥¼ ì‹œê°í™” í•œê²°ê³¼, ì¼ë¶€ í•„í„°ê°€ ì§€ë‚˜ì¹˜ê²Œ ì§€ë°°ì ì´ê¸° ë•Œë¬¸ì— ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ **RMSê°’ì´ 10^-1ì„ ì´ˆê³¼í•˜ëŠ” í•„í„°ëŠ” ê°•ì œë¡œ renormalization(ì¬ì •ê·œí™”, 0.1ë¡œ ë§Œë“¦, ê· í˜• ìœ ì§€)**í•œë‹¤. ì´ëŠ” [-128, 128]ì¸ 1ì¸µì—ì„œ ì¤‘ìš”í•˜ë‹¤.

### âœ… 5ì¤„ ìš”ì•½

- ImageNet 2012ì—ì„œ í•™ìŠµ (130ë§Œ ì¥, 1000 í´ë˜ìŠ¤)
- ì „ì²˜ë¦¬: ë¦¬ì‚¬ì´ì¦ˆÂ·í¬ë¡­Â·í‰ê·  ì œê±°Â·ë°ì´í„° ì¦ê°•
- í•™ìŠµ: SGD, mini-batch=128, learning rate=0.01 ì‹œì‘, momentum=0.9, Dropout=0.5
- í•„í„° ì¬ì •ê·œí™”ë¡œ íŠ¹ì • í•„í„° ì§€ë°° ë°©ì§€
- 70 epoch, GPU 1ì¥, ì•½ 12ì¼ ì†Œìš”

---

### ğŸ“Œ ì •ë¦¬

| í•­ëª©      | ì„¤ì •                                                             |
| --------- | ---------------------------------------------------------------- |
| ë°ì´í„°    | ImageNet 2012 (1.3M, 1000 í´ë˜ìŠ¤)                                |
| ì „ì²˜ë¦¬    | ë¦¬ì‚¬ì´ì¦ˆ 256, ì¤‘ì•™ í¬ë¡­, í‰ê·  ì œê±°, 224x224 ì„œë¸Œ í¬ë¡­ 10ê°œ, flip |
| ìµœì í™”    | SGD, batch=128, lr=0.01, momentum=0.9                            |
| ì •ê·œí™”    | Dropout(0.5), filter RMS clipping(0.1)                           |
| êµ¬ì¡° ì°¨ì´ | AlexNet sparse â†’ dense ì—°ê²°                                      |
| í•™ìŠµ ì‹œê°„ | 70 epoch, GTX580 GPU, 12ì¼                                       |
</aside>

### ğŸ“šÂ 4. Convnet Visualization

### ë²ˆì—­

1. ì›ë¬¸ (Section 4 ë°œì·Œ)
    
    Using the model described in Section 3, we now use the deconvnet to visualize the feature activations on the ImageNet validation set.
    
    **Feature Visualization:** Fig. 2 shows feature visualizations from our model once training is complete. However, instead of showing the single strongest activation for a given feature map, we show the top 9 activations. Projecting each separately down to pixel space reveals the different structures that excite a given feature map, hence showing its invariance to input deformations. Alongside these visualizations we show the corresponding image patches. â€¦ For example, in layer 5, row 1, col 2, the patches appear to have little in common, but the visualizations reveal that this particular feature map focuses on the grass in the background, not the foreground objects.
    
    The projections from each layer show the hierarchical nature of the features in the network. Layer 2 responds to corners and other edge/color conjunctions. Layer 3 has more complex invariances, capturing similar textures. Layer 4 shows significant variation, but is more class-specific (e.g. dog faces, bird legs). Layer 5 shows entire objects with significant pose variation (e.g. keyboards, dogs).
    
    **Feature Evolution during Training:** Fig. 4 visualizes the progression during training of the strongest activation â€¦ The lower layers converge within a few epochs, but the upper layers only develop after 40â€“50 epochs.
    
    **Feature Invariance:** Fig. 5 shows how features change under translation, rotation, and scaling. Small transformations affect the first layer strongly, but higher layers are more stable, showing quasi-linear changes.
    

---

1. ë²ˆì—­ë¬¸ (ConvNet ì‹œê°í™”)
    
    Section 3ì—ì„œ ì„¤ëª…í•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬, ì´ì œ ìš°ë¦¬ëŠ” deconvnetì„ í†µí•´ ImageNet ê²€ì¦ì…‹ì˜ feature activationì„ ì‹œê°í™”í•œë‹¤.
    
    **íŠ¹ì§• ì‹œê°í™” (Feature Visualization):** Fig. 2ëŠ” í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ì˜ íŠ¹ì§• ì‹œê°í™” ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤. ê° feature mapì—ì„œ ë‹¨ì¼ ìµœê°• í™œì„±í™”ë§Œ ë³´ì—¬ì£¼ëŠ” ëŒ€ì‹ , ìƒìœ„ 9ê°œì˜ activationì„ í‘œì‹œí•˜ì˜€ë‹¤. ê°ê°ì„ í”½ì…€ ê³µê°„ìœ¼ë¡œ íˆ¬ì˜í•˜ë©´ í•´ë‹¹ feature mapì„ í¥ë¶„ì‹œí‚¤ëŠ” ë‹¤ì–‘í•œ êµ¬ì¡°ê°€ ë“œëŸ¬ë‚˜ë©°, ì´ë¥¼ í†µí•´ ì…ë ¥ ë³€í˜•(input deformation)ì— ëŒ€í•œ ë¶ˆë³€ì„±(invariance)ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ì™€ í•¨ê»˜ í•´ë‹¹ë˜ëŠ” ì›ë³¸ ì´ë¯¸ì§€ íŒ¨ì¹˜ë„ ë‚˜ë€íˆ ë³´ì—¬ì¤€ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 5ì¸µ(layer 5)ì˜ (row 1, col 2)ì—ì„œëŠ” ì›ë³¸ íŒ¨ì¹˜ë“¤ì´ ì„œë¡œ í¬ê²Œ ë‹¤ë¥´ì§€ë§Œ, ì‹œê°í™”ëŠ” í•´ë‹¹ feature mapì´ **ì „ê²½ ê°ì²´ê°€ ì•„ë‹ˆë¼ ë°°ê²½ì˜ í’€(grasstexture)**ì— ì£¼ëª©í•œë‹¤ëŠ” ê²ƒì„ ë“œëŸ¬ë‚¸ë‹¤.
    
    ê° ì¸µì—ì„œ ì–»ì€ projectionì€ ë„¤íŠ¸ì›Œí¬ íŠ¹ì§•ì˜ **ê³„ì¸µì  ì„±ê²©(hierarchical nature)**ì„ ë³´ì—¬ì¤€ë‹¤.
    
    - 2ì¸µ(layer 2): ëª¨ì„œë¦¬(corner), ìƒ‰ìƒ/ì—£ì§€ ê²°í•© êµ¬ì¡°ì— ë°˜ì‘
    - 3ì¸µ(layer 3): í…ìŠ¤ì²˜(texture)ì™€ ê°™ì€ ë³µì¡í•œ ë¶ˆë³€ì„± íŒ¨í„´ í¬ì°©
    - 4ì¸µ(layer 4): í´ë˜ìŠ¤ íŠ¹ì´ì (class-specific) íŒ¨í„´ (ì˜ˆ: ê°œ ì–¼êµ´, ìƒˆ ë‹¤ë¦¬)
    - 5ì¸µ(layer 5): í¬ì¦ˆ ë³€í™”ê°€ í° ì „ì²´ ê°ì²´ (ì˜ˆ: í‚¤ë³´ë“œ, ê°œ ì „ì²´ ëª¨ìŠµ)
    
    **í›ˆë ¨ ì¤‘ íŠ¹ì§•ì˜ ì§„í™” (Feature Evolution during Training):** Fig. 4ëŠ” í›ˆë ¨ ê³¼ì •ì—ì„œ strongest activationì„ ì‹œê°í™”í•œë‹¤. í•˜ìœ„ ê³„ì¸µì€ ëª‡ epoch ë‚´ì— ìˆ˜ë ´í•˜ì§€ë§Œ, ìƒìœ„ ê³„ì¸µì€ 40~50 epoch ì´í›„ì—ì•¼ ë°œë‹¬í•œë‹¤.
    
    **íŠ¹ì§• ë¶ˆë³€ì„± (Feature Invariance):** Fig. 5ëŠ” íŠ¹ì§•ì´ í‰í–‰ ì´ë™(translation), íšŒì „(rotation), í¬ê¸° ë³€í™”(scaling)ì— ë”°ë¼ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ë¥¼ ë³´ì—¬ì¤€ë‹¤. ì‘ì€ ë³€í™˜ì€ 1ì¸µì—ì„œ í° ì˜í–¥ì„ ì£¼ì§€ë§Œ, ìƒìœ„ ì¸µìœ¼ë¡œ ê°ˆìˆ˜ë¡ ë” ì•ˆì •ì (quasi-linear)ì¸ ë°˜ì‘ì„ ë³´ì¸ë‹¤.
    
2. ì›ë¬¸ (Section 4.1 ë°œì·Œ)
    
    While visualization of a trained model gives insight into its operation, it can also assist with selecting good architectures in the first place. By visualizing the first and second layers of Krizhevsky et al.â€™s architecture (Fig. 6(b) & (d)), various problems are apparent. The first layer filters are a mix of extremely high and low frequency information, with little coverage of the mid frequencies. Additionally, the 2nd layer visualization shows aliasing artifacts caused by the large stride 4 used in the 1st layer convolutions.
    
    To remedy these problems, we (i) reduced the 1st layer filter size from 11x11 to 7x7 and (ii) made the stride of the convolution 2, rather than 4. This new architecture retains much more information in the 1st and 2nd layer features, as shown in Fig. 6(c) & (e). More importantly, it also improves the classification performance as shown in Section 5.1.
    

---

1. ë²ˆì—­ë¬¸ (4.1 ì•„í‚¤í…ì²˜ ì„ íƒ)
    
    í›ˆë ¨ëœ ëª¨ë¸ì˜ ì‹œê°í™”ëŠ” ë‚´ë¶€ ë™ì‘ì— ëŒ€í•œ í†µì°°ì„ ì¤„ ë¿ ì•„ë‹ˆë¼, ì²˜ìŒë¶€í„° ë” ì¢‹ì€ ì•„í‚¤í…ì²˜ë¥¼ ì„ íƒí•˜ëŠ” ë°ì—ë„ ë„ì›€ì´ ëœë‹¤. Krizhevsky et al.ì˜ ì•„í‚¤í…ì²˜ì˜ 1ì¸µê³¼ 2ì¸µì„ ì‹œê°í™”í•œ ê²°ê³¼(Fig. 6(b), (d)), ëª‡ ê°€ì§€ ë¬¸ì œê°€ ë“œëŸ¬ë‚¬ë‹¤.
    
    - **1ì¸µ í•„í„°**: ë§¤ìš° ê³ ì£¼íŒŒ(high frequency)ì™€ ì €ì£¼íŒŒ(low frequency) ì •ë³´ê°€ í˜¼í•©ë˜ì–´ ìˆìœ¼ë©°, **ì¤‘ê°„ ì£¼íŒŒìˆ˜(mid frequency) ì˜ì—­**ì˜ ì»¤ë²„ë¦¬ì§€ê°€ ë¶€ì¡±í•˜ë‹¤.
    - **2ì¸µ íŠ¹ì§•**: 1ì¸µ í•©ì„±ê³±ì—ì„œ stride=4ë¥¼ ì‚¬ìš©í•œ íƒ“ì— **aliasing artifact(ìƒ˜í”Œë§ ì™œê³¡)**ê°€ ë‚˜íƒ€ë‚œë‹¤.
    
    ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì €ìë“¤ì€:
    
    1. **1ì¸µ í•„í„° í¬ê¸°**ë¥¼ 11x11ì—ì„œ 7x7ë¡œ ì¤„ì´ê³ ,
    2. **í•©ì„±ê³± stride**ë¥¼ 4ì—ì„œ 2ë¡œ ì¶•ì†Œí•˜ì˜€ë‹¤.
    
    ì´ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ëŠ” Fig. 6(c), (e)ì— ë³´ì´ë“¯ì´ 1ì¸µê³¼ 2ì¸µì—ì„œ í›¨ì”¬ ë” ë§ì€ ì •ë³´ë¥¼ ë³´ì¡´í•œë‹¤. ë” ì¤‘ìš”í•œ ê²ƒì€, ì´ëŸ¬í•œ ë³€ê²½ì´ ImageNet ë¶„ë¥˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤ëŠ” ì ì´ë‹¤(Section 5.1 ì°¸ê³ ).
    
2. ì›ë¬¸ (Section 4.2 ë°œì·Œ)
    
    With image classification approaches, a natural question is if the model is truly identifying the location of the object in the image, or just using the surrounding context. Fig. 7 attempts to answer this question by systematically occluding different portions of the input image with a grey square, and monitoring the output of the classifier. The examples clearly show the model is localizing the objects within the scene, as the probability of the correct class drops significantly when the object is occluded.
    
    Fig. 7 also shows visualizations from the strongest feature map of the top convolution layer, in addition to activity in this map (summed over spatial locations) as a function of occluder position. When the occluder covers the image region that appears in the visualization, we see a strong drop in activity in the feature map. This shows that the visualization genuinely corresponds to the image structure that stimulates that feature map, hence validating the other visualizations shown in Fig. 4 and Fig. 2.
    

---

1. ë²ˆì—­ë¬¸ (4.2 Occlusion Sensitivity)
    
    ì´ë¯¸ì§€ ë¶„ë¥˜ ì ‘ê·¼ë²•ì—ì„œ ìì—°ìŠ¤ëŸ¬ìš´ ì˜ë¬¸ì€, ëª¨ë¸ì´ ì‹¤ì œë¡œ ì´ë¯¸ì§€ ì† ê°ì²´ì˜ ìœ„ì¹˜ë¥¼ ì¸ì‹í•˜ëŠ”ì§€, ì•„ë‹ˆë©´ ë‹¨ìˆœíˆ ì£¼ë³€ ë§¥ë½(context)ë§Œ ì‚¬ìš©í•˜ëŠ”ì§€ì´ë‹¤. Fig. 7ì€ ì…ë ¥ ì´ë¯¸ì§€ì˜ ì„œë¡œ ë‹¤ë¥¸ ì˜ì—­ì„ íšŒìƒ‰ ì •ì‚¬ê°í˜•ìœ¼ë¡œ **ì²´ê³„ì ìœ¼ë¡œ ê°€ë ¤(occlusion)** ë³´ë©´ì„œ ë¶„ë¥˜ê¸°ì˜ ì¶œë ¥ì„ ê´€ì°°í•˜ì—¬ ì´ ì§ˆë¬¸ì— ë‹µí•˜ê³ ì í•œë‹¤.
    
    ê·¸ ê²°ê³¼, ê°ì²´ê°€ ê°€ë ¤ì§€ë©´ ì˜¬ë°”ë¥¸ í´ë˜ìŠ¤ í™•ë¥ ì´ í¬ê²Œ ë–¨ì–´ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ì¥ë©´ ë‚´ì—ì„œ ì‹¤ì œ ê°ì²´ ìœ„ì¹˜ë¥¼ ì°¾ì•„ë‚´ê³  ìˆë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.
    
    ë˜í•œ Fig. 7ì€ ìµœìƒìœ„ í•©ì„±ê³± ê³„ì¸µ(top convolutional layer)ì˜ ê°€ì¥ ê°•ë ¥í•œ feature mapì˜ ì‹œê°í™”ë¥¼ í•¨ê»˜ ë³´ì—¬ì£¼ëŠ”ë°, ì´ feature mapì˜ í™œì„±í™”(ê³µê°„ ì „ì²´ í•©)ë¥¼ occluder ìœ„ì¹˜ì— ë”°ë¼ ì¸¡ì •í•˜ì˜€ë‹¤. ê°€ë ¤ì§„ ë¶€ë¶„ì´ ì‹œê°í™”ì—ì„œ ë‚˜íƒ€ë‚œ ê°ì²´ êµ¬ì¡°ì™€ ê²¹ì¹˜ë©´ feature map í™œì„±ë„ê°€ í¬ê²Œ ë–¨ì–´ì¡Œë‹¤. ì´ëŠ” ì‹œê°í™”ê°€ ì‹¤ì œë¡œ í•´ë‹¹ feature mapì„ ìê·¹í•˜ëŠ” ì´ë¯¸ì§€ êµ¬ì¡°ì™€ ì •í™•íˆ ëŒ€ì‘í•˜ê³  ìˆìŒì„ ë³´ì—¬ì£¼ë©°, Fig. 4ì™€ Fig. 2ì—ì„œ ì œì‹œëœ ë‹¤ë¥¸ ì‹œê°í™” ê²°ê³¼ë¥¼ ë’·ë°›ì¹¨í•œë‹¤.
    
2. ì›ë¬¸ (Section 4.3 ë°œì·Œ)
    
    Deep models differ from many existing recognition approaches in that there is no explicit mechanism for establishing correspondence between specific object parts in different images (e.g. faces have a particular spatial configuration of the eyes and nose). However, an intriguing possibility is that deep models might be implicitly computing them. To explore this, we take 5 randomly drawn dog images with frontal pose and systematically mask out the same part of the face in each image (e.g. all left eyes, see Fig. 8). For each image i, we then compute: Îµli = xli âˆ’ xÌƒli, where xli and xÌƒli are the feature vectors at layer l for the original and occluded images respectively. We then measure the consistency of this difference vector Îµ between all related image pairs (i, j): Î”l = âˆ‘ H(sign(Îµli), sign(Îµlj)), where H is Hamming distance. A lower value indicates greater consistency in the change resulting from the masking operation, hence tighter correspondence between the same object parts in different images.
    
    In Table 1 we compare the Î” score for three parts of the face (left eye, right eye and nose) to random parts of the object, using features from layer l = 5 and l = 7. The lower score for these parts, relative to random object regions, for the layer 5 features show the model does establish some degree of correspondence.
    

---

1. ë²ˆì—­ë¬¸ (4.3 ëŒ€ì‘ ë¶„ì„)
    
    ê¸°ì¡´ì˜ ë§ì€ ì¸ì‹ ì ‘ê·¼ë²•ì€ ì„œë¡œ ë‹¤ë¥¸ ì´ë¯¸ì§€ì—ì„œ **íŠ¹ì • ê°ì²´ ë¶€ìœ„ ê°„ì˜ ëŒ€ì‘(correspondence)**ì„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•œë‹¤ (ì˜ˆ: ì–¼êµ´ì€ ëˆˆê³¼ ì½”ê°€ íŠ¹ì • ê³µê°„ì  ë°°ì—´ì„ ê°€ì§). ë°˜ë©´, ë”¥ ëª¨ë¸ì€ ì´ëŸ¬í•œ ë©”ì»¤ë‹ˆì¦˜ì„ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤. í•˜ì§€ë§Œ í¥ë¯¸ë¡œìš´ ê°€ëŠ¥ì„±ì€, ë”¥ ëª¨ë¸ì´ ì•”ë¬µì ìœ¼ë¡œ ì´ëŸ¬í•œ ëŒ€ì‘ì„ í•™ìŠµí•˜ê³  ìˆì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.
    
    ì´ë¥¼ íƒêµ¬í•˜ê¸° ìœ„í•´, ì •ë©´(frontal pose) ìƒíƒœì˜ ê°œ(dog) ì´ë¯¸ì§€ 5ì¥ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ê³ , ëª¨ë“  ì´ë¯¸ì§€ì—ì„œ ë™ì¼í•œ ì–¼êµ´ ë¶€ìœ„(ì˜ˆ: ì™¼ìª½ ëˆˆ)ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë§ˆìŠ¤í‚¹í•˜ì˜€ë‹¤(Fig. 8 ì°¸ì¡°). ê° ì´ë¯¸ì§€ iì— ëŒ€í•´, ì›ë³¸ ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ëœ ì´ë¯¸ì§€ì˜ layer l íŠ¹ì§• ë²¡í„°ë¥¼ ê°ê° xli, xÌƒlië¼ í•˜ë©´, ê·¸ ì°¨ì´ Îµli = xli âˆ’ xÌƒli ë¥¼ ê³„ì‚°í•œë‹¤. ì´í›„, ëª¨ë“  ê´€ë ¨ ì´ë¯¸ì§€ ìŒ (i, j)ì— ëŒ€í•´ ì°¨ì´ ë²¡í„° Îµì˜ ì¼ê´€ì„±ì„ í•´ë° ê±°ë¦¬(Hamming distance) ê¸°ë°˜ìœ¼ë¡œ ì¸¡ì •í•œë‹¤:
    
    Î”l = âˆ‘ H(sign(Îµli), sign(Îµlj))
    
    Î” ê°’ì´ ë‚®ì„ìˆ˜ë¡ ë™ì¼í•œ ë¶€ìœ„ë¥¼ ë§ˆìŠ¤í‚¹í–ˆì„ ë•Œì˜ ë³€í™”ê°€ ë” ì¼ê´€ì ì´ë¼ëŠ” ì˜ë¯¸ì´ë©°, ë”°ë¼ì„œ ë‹¤ë¥¸ ì´ë¯¸ì§€ ê°„ì— í•´ë‹¹ ë¶€ìœ„ê°€ ë” ê°•í•˜ê²Œ ëŒ€ì‘ë˜ê³  ìˆìŒì„ ë‚˜íƒ€ë‚¸ë‹¤.
    
    Table 1ì€ ì„¸ ê°€ì§€ ì–¼êµ´ ë¶€ìœ„(ì™¼ìª½ ëˆˆ, ì˜¤ë¥¸ìª½ ëˆˆ, ì½”)ì™€ ë¬´ì‘ìœ„ ë¶€ìœ„ë¥¼ ë¹„êµí•œ ê²°ê³¼ì´ë‹¤. Layer 5 íŠ¹ì§•ì—ì„œëŠ” ë¬´ì‘ìœ„ ë¶€ìœ„ì— ë¹„í•´ ëˆˆê³¼ ì½”ì—ì„œ Î” ê°’ì´ ë” ë‚®ê²Œ ë‚˜íƒ€ë‚¬ìœ¼ë©°, ì´ëŠ” ëª¨ë¸ì´ ì¼ì • ìˆ˜ì¤€ì˜ ëŒ€ì‘ì„±ì„ í˜•ì„±í•˜ê³  ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤. Layer 7ì—ì„œëŠ” ì´ ì°¨ì´ê°€ ì¤„ì–´ë“œëŠ”ë°, ì´ëŠ” ìƒìœ„ì¸µì´ ì£¼ë¡œ **í´ë˜ìŠ¤ íŒë³„(breed discrimination)**ì— ì§‘ì¤‘í•˜ê¸° ë•Œë¬¸ìœ¼ë¡œ ë³´ì¸ë‹¤.
    

<aside>

![image.png]((ZFNet)%20Visualizing%20and%20Understanding%20Convolutiona%2025ca9b246de18068af2bc450e4cf0526/image%202.png)

# 4. ì‹œê°í™”

- ê° ì¸µë³„ íŠ¹ì§• : ê·¸ ë ˆì´ì–´ì—ì„œì˜ ë³µì›ì´ ì•„ë‹ˆë¼, ì—­ìœ¼ë¡œ ì „ë¶€ë‹¤ ê±°ì¹œ í›„ ë³µì›, ë”°ë¼ì„œ ê¹Šì–´ì§ˆìˆ˜ë¡ í•´ìƒë„ê°€ ë†’ì•„ì§„ë‹¤. e.g. 2ì¸µì˜ ê²½ìš° 2 â†’ 1, 5ì¸µì˜ ê²½ìš° 5 â†’ 4 â†’ â€¦ â†’ 1 ì¦‰ í•˜ìœ„ê³„ì¸µ(ì´ˆê¸°ì¸µ)ê³¼ ìƒìœ„ê³„ì¸µ(í›„ë°˜ì¸µ)ì„ ë¹„êµ
    - 2ì¸µ(layer 2): ëª¨ì„œë¦¬(corner), ìƒ‰ìƒ/ì—£ì§€ ê²°í•© êµ¬ì¡°ì— ë°˜ì‘
    - 3ì¸µ(layer 3): í…ìŠ¤ì²˜(texture)ì™€ ê°™ì€ ë³µì¡í•œ ë¶ˆë³€ì„± íŒ¨í„´ í¬ì°©
    - 4ì¸µ(layer 4): í´ë˜ìŠ¤ íŠ¹ì´ì (class-specific) íŒ¨í„´ (ì˜ˆ: ê°œ ì–¼êµ´, ìƒˆ ë‹¤ë¦¬)
    - 5ì¸µ(layer 5): í¬ì¦ˆ ë³€í™”ê°€ í° ì „ì²´ ê°ì²´ (ì˜ˆ: í‚¤ë³´ë“œ, ê°œ ì „ì²´ ëª¨ìŠµ)
- **ì…ë ¥ ë³€í˜•(input deformation)ì— ëŒ€í•œ ë¶ˆë³€ì„±(invariance)ì„ í™•ì¸ â†’ ì‘ì€ ë³€í™”ëŠ” í•˜ìœ„ì¸µì—ì„œ í° íš¨ê³¼ë¥¼ ì£¼ì§€ë§Œ ìƒìœ„ì¸µì—ëŠ” quasi-linear(ì•ˆì •ì )ì¸ ë°˜ì‘ì„ ë³´ì—¬ì¤Œ**
    
    ![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-08-28 á„‹á…©á„Œá…¥á†« 12.47.55.jpg]((ZFNet)%20Visualizing%20and%20Understanding%20Convolutiona%2025ca9b246de18068af2bc450e4cf0526/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2025-08-28_%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB_12.47.55.jpg)
    
- **ê³„ì¸µì  ì„±ê²©(hierarchical nature)**
- **í•˜ìœ„ ê³„ì¸µì€ ì†Œìˆ˜ì˜ epochsë§Œì— ìˆ˜ë ´, ìƒìœ„ëŠ” ì˜¤ë˜ê±¸ë¦¼**
    
    ![image.png]((ZFNet)%20Visualizing%20and%20Understanding%20Convolutiona%2025ca9b246de18068af2bc450e4cf0526/image%203.png)
    
- **ì¦‰ ConvNetì€ ê¹Šê²Œ í•™ìŠµë ìˆ˜ë¡ ì¶”ìƒì ì¸ íŠ¹ì§•ì„ í•™ìŠµ**

### âœ… 5ì¤„ ìš”ì•½

- Deconvnetìœ¼ë¡œ ê° ì¸µì˜ feature mapì„ í”½ì…€ ê³µê°„ìœ¼ë¡œ ë³µì›
- í•˜ìœ„ì¸µ: ì €ìˆ˜ì¤€ íŠ¹ì§•(ì—£ì§€, ì½”ë„ˆ), ì¤‘ê°„ì¸µ: í…ìŠ¤ì²˜, ìƒìœ„ì¸µ: ê°ì²´/ë¶€ìœ„
- í›ˆë ¨ ì´ˆê¸°ì— í•˜ìœ„ì¸µì€ ë¹ ë¥´ê²Œ ìˆ˜ë ´, ìƒìœ„ì¸µì€ ëŠ¦ê²Œ ë°œë‹¬
- ì…ë ¥ ë³€í˜•ì— ëŒ€í•´ ìƒìœ„ì¸µì€ ë” ì•ˆì •ì  â†’ ë¶ˆë³€ì„± í™•ë³´
- ConvNetì€ ê³„ì¸µì ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” í‘œí˜„ì„ í•™ìŠµí•¨ì„ ì‹¤ì¦

---

### ğŸ“Œ ì •ë¦¬

| ì¸µ (Layer) | ì£¼ìš” íŠ¹ì§•          | ì‹œê°í™” ê²°ê³¼               |
| ---------- | ------------------ | ------------------------- |
| Layer 2    | ì½”ë„ˆ, ì—£ì§€+ìƒ‰ ê²°í•© | ê¸°ë³¸ ê¸°í•˜í•™ì  êµ¬ì¡° ê°ì§€   |
| Layer 3    | í…ìŠ¤ì²˜, ë°˜ë³µ íŒ¨í„´  | ë©”ì‹œ(mesh), í…ìŠ¤íŠ¸ ì¸ì‹   |
| Layer 4    | í´ë˜ìŠ¤ íŠ¹ì´ì  ë¶€ìœ„ | ê°œ ì–¼êµ´, ìƒˆ ë‹¤ë¦¬ ë“±       |
| Layer 5    | ì „ì²´ ê°ì²´          | ê°œ, í‚¤ë³´ë“œ ë“± ë‹¤ì–‘í•œ í¬ì¦ˆ |

## 4.1. Architecture Selection

- AlexNetì˜ ë¬¸ì œì 
    - 1ì¸µ : ë§¤ìš° ê³ ì£¼íŒŒ(high frequency)ì™€ ì €ì£¼íŒŒ(low frequency) ì •ë³´ê°€ í˜¼í•©ë˜ì–´ ìˆìœ¼ë©°, **ì¤‘ê°„ ì£¼íŒŒìˆ˜(mid frequency) ì˜ì—­ì˜  ì»¤ë²„ê°€ ë¶€ì¡±**
    - 2ì¸µ : 1ì¸µ í•©ì„±ê³±ì—ì„œ stride=4ë¥¼ ì‚¬ìš©í•œ íƒ“ì— **aliasing artifact(ìƒ˜í”Œë§ ì™œê³¡)**ì´ ë°œìƒ
- ZFNet ìˆ˜ì •ì‚¬í•­
    - **1ì¸µ í•„í„° í¬ê¸°**ë¥¼ 11x11ì—ì„œ 7x7ë¡œ ì¤„ì´ê³ ,
    - **í•©ì„±ê³± stride**ë¥¼ 4ì—ì„œ 2ë¡œ ì¶•ì†Œí•˜ì˜€ë‹¤.

![image.png]((ZFNet)%20Visualizing%20and%20Understanding%20Convolutiona%2025ca9b246de18068af2bc450e4cf0526/image%204.png)

### âœ… 5ì¤„ ìš”ì•½

- ì‹œê°í™”ë¥¼ í†µí•´ AlexNetì˜ 1Â·2ì¸µì—ì„œ ë¬¸ì œì  ë°œê²¬
- 1ì¸µ: ì¤‘ê°„ ì£¼íŒŒìˆ˜ ë¶€ì¡±, 2ì¸µ: stride=4ë¡œ ì¸í•œ aliasing
- ê°œì„ : 11x11 í•„í„° â†’ 7x7, stride=4 â†’ 2
- ê²°ê³¼: ì •ë³´ ë³´ì¡´ â†‘, aliasing â†“
- ì„±ëŠ¥ ë˜í•œ ê°œì„ ë¨ (Section 5.1)

---

### ğŸ“Œ ì •ë¦¬

| ë¬¸ì œ (AlexNet)                          | í•´ê²°ì±… (Zeiler & Fergus) | ê²°ê³¼                       |
| --------------------------------------- | ------------------------ | -------------------------- |
| 1ì¸µ: ê³ Â·ì €ì£¼íŒŒ ìœ„ì£¼, mid-frequency ë¶€ì¡± | í•„í„° í¬ê¸° 11x11 â†’ 7x7    | ë” ê· í˜• ì¡íŒ í•„í„°          |
| 2ì¸µ: stride=4 â†’ aliasing ë°œìƒ           | stride=4 â†’ 2             | aliasing ì œê±°, ì •ë³´ ë³´ì¡´ â†‘ |
| ì„±ëŠ¥                                    | ê°œì„  ì „ AlexNet          | ê°œì„  í›„ ë” ë‚®ì€ ì˜¤ë¥˜ìœ¨     |

## 4.2 Occlusion Sensitivity

- ê°ì²´ì˜ ìœ„ì¹˜ë¥¼ ì¸ì‹í•˜ëŠ”ì§€, ì£¼ë³€ ë§¥ë½ë§Œ ì‚¬ìš©í•˜ëŠ”ì§€ë¥¼ í™•ì¸í•´ë³´ëŠ” ì‹¤í—˜ìœ¼ë¡œ, ê²°ë¡ ì ìœ¼ë¡œ **ê°ì²´ë¥¼ ì§€ìš°ëŠ” ê²½ìš° ì˜¬ë°”ë¥¸ í´ë˜ìŠ¤ í™•ë¥ ì´ í¬ê²Œ ë–¨ì–´ì§„ë‹¤**.
- ì¦‰ ëª¨ë¸ì€ **ê°ì²´ ìì²´ì— ì§‘ì¤‘í•˜ì—¬,** ê°ì²´ íƒì§€ì˜ ê°€ëŠ¥ì„±ì— ëŒ€í•´ ì‹œì‚¬

![image.png]((ZFNet)%20Visualizing%20and%20Understanding%20Convolutiona%2025ca9b246de18068af2bc450e4cf0526/image%205.png)

### âœ… 5ì¤„ ìš”ì•½

- Occlusionìœ¼ë¡œ ëª¨ë¸ì´ ë°°ê²½ì´ ì•„ë‹Œ ê°ì²´ ìœ„ì¹˜ì— ì˜ì¡´í•¨ì„ í™•ì¸
- ê°ì²´ ë¶€ìœ„ê°€ ê°€ë ¤ì§€ë©´ ì˜¬ë°”ë¥¸ í´ë˜ìŠ¤ í™•ë¥  ê¸‰ë½
- top conv layerì˜ feature map í™œì„±ë„ë„ í•¨ê»˜ ê¸‰ë½
- Deconvnet ì‹œê°í™”ê°€ ì§„ì§œ ìê·¹ êµ¬ì¡°ë¥¼ ë³´ì—¬ì¤€ë‹¤ëŠ” ê²€ì¦
- ConvNetì€ ì¥ë©´ contextë³´ë‹¤ ê°ì²´ local structureì— ë¯¼ê°

---

### ğŸ“Œ ì •ë¦¬

| ì§ˆë¬¸                                       | ë°©ë²•                                   | ê²°ê³¼                                      | ì˜ë¯¸                                        |
| ------------------------------------------ | -------------------------------------- | ----------------------------------------- | ------------------------------------------- |
| ê°ì²´ ìœ„ì¹˜ë¥¼ ë³´ëŠ”ê°€, ë°°ê²½ contextë¥¼ ë³´ëŠ”ê°€? | ì´ë¯¸ì§€ ì˜ì—­ì„ ìˆœì°¨ì ìœ¼ë¡œ ê°€ë¦¼          | ê°ì²´ ë¶€ë¶„ì´ ê°€ë ¤ì§€ë©´ í™•ë¥  ê¸‰ë½            | ConvNetì€ ê°ì²´ local structureì— ì§‘ì¤‘       |
| ì‹œê°í™” ì‹ ë¢°ì„± ê²€ì¦                         | top conv layer feature map í™œì„±ë„ ê´€ì°° | occluderê°€ í•´ë‹¹ êµ¬ì¡°ë¥¼ ê°€ë¦¬ë©´ í™œì„±ë„ ê¸‰ë½ | Deconvnet ì‹œê°í™” ê²°ê³¼ê°€ ì‹¤ì œ featureì™€ ì¼ì¹˜ |

## 4.3. Correspondence Analysis

- ê¸°ì¡´ì˜ ëª¨ë¸ë“¤ì€ **íŠ¹ì • ê°ì²´ ë¶€ìœ„ê°„ì˜ ëŒ€ì‘(ì–¼êµ´ ì†, ì½”ì™€ ëˆˆ)**ì„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•˜ê²Œ ë˜ëŠ”ë°, ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ê²½ìš°ì—ëŠ” ì´ëŸ¬í•œ ë©”ì»¤ë‹ˆì¦˜ì´ **ì•”ë¬µì ìœ¼ë¡œ ëŒ€ì‘**í•œë‹¤ëŠ” ì ì´ë‹¤.

âœ… ë°©ë²•

1. ê°œ ì–¼êµ´ ì´ë¯¸ì§€ 5ì¥ ì„ íƒ
2. ë™ì¼í•œ ìœ„ì¹˜(ì˜ˆ: ì™¼ìª½ ëˆˆ)ë¥¼ ê°€ë ¤ì„œ ì›ë³¸ê³¼ feature ì°¨ì´ Îµli ê³„ì‚°
3. ì´ë¯¸ì§€ ìŒ(i, j) ê°„ì˜ ì°¨ì´ ë²¡í„° ì¼ê´€ì„±ì„ í•´ë° ê±°ë¦¬ë¡œ ì¸¡ì • (Î”l)
4. íŠ¹ì • ë¶€ìœ„ vs ë¬´ì‘ìœ„ ë¶€ìœ„ ë¹„êµ

âœ… ê²°ê³¼

- Layer 5: ëˆˆÂ·ì½” ê°™ì€ ì˜ë¯¸ ìˆëŠ” ë¶€ìœ„ì—ì„œ Î” ê°’ì´ ë‚®ìŒ â†’ ëŒ€ì‘ì„± í™•ë³´
- Layer 7: breed íŒë³„ì— ì§‘ì¤‘í•˜ë¯€ë¡œ Î” ê°’ì´ ë¬´ì‘ìœ„ ë¶€ìœ„ì™€ ìœ ì‚¬ â†’ ë¶€ìœ„ ëŒ€ì‘ ì •ë³´ ì•½í™”

âœ… ì˜ë¯¸

- ConvNetì€ ëª…ì‹œì ìœ¼ë¡œ correspondenceë¥¼ ì •ì˜í•˜ì§€ ì•Šì•„ë„, ì¤‘ê°„ì¸µì—ì„œ **ê°ì²´ ë¶€ìœ„ ê°„ ì•”ë¬µì  ëŒ€ì‘**ì„ í•™ìŠµ
- ê·¸ëŸ¬ë‚˜ ê¹Šì€ ì¸µìœ¼ë¡œ ê°ˆìˆ˜ë¡ ì´ ì •ë³´ëŠ” ì‚¬ë¼ì§€ê³ , **í´ë˜ìŠ¤ êµ¬ë¶„ì— ë” íŠ¹í™”**ë¨

<aside>

### í•´ë° ê±°ë¦¬

ë‘ ë²¡í„°ê°€ ìˆì„ë•Œ, ì„œë¡œ ë‹¤ë¥¸ ìœ„ì¹˜ì˜ ì›ì†Œ ê°œìˆ˜ë¥¼ ìƒˆëŠ” ê±°ë¦¬ ì¸¡ë„

ê°’ì´ ì‘ì„ìˆ˜ë¡ ì„œë¡œ ë‹¤ë¥¸ ì´ë¯¸ì§€ì—ì„œë„ ê°™ì€ ë¶€ìœ„ê°€ ê³µí†µëœ ì—­í• ì„ í•˜ê³  ìˆìŒì„ ì˜ë¯¸

</aside>

![image.png]((ZFNet)%20Visualizing%20and%20Understanding%20Convolutiona%2025ca9b246de18068af2bc450e4cf0526/image%206.png)

### âœ… 5ì¤„ ìš”ì•½

- ConvNetì´ ì•”ë¬µì ìœ¼ë¡œ ê°ì²´ ë¶€ìœ„ ëŒ€ì‘ì„ í•™ìŠµí•˜ëŠ”ì§€ ì‹¤í—˜
- ë™ì¼ ë¶€ìœ„ë¥¼ ê°€ë ¤ feature ë³€í™”ëŸ‰ì„ ë¹„êµ
- Layer 5: ëˆˆÂ·ì½”ì—ì„œ ë³€í™”ê°€ ì¼ê´€ì  â†’ ëŒ€ì‘ì„± ì¡´ì¬
- Layer 7: breed íŒë³„ë¡œ ì¹˜ì¤‘ â†’ ëŒ€ì‘ì„± ê°ì†Œ
- ConvNetì€ ì¤‘ê°„ì¸µì—ì„œ correspondenceë¥¼ í˜•ì„±í•˜ì§€ë§Œ, ê¹Šì€ ì¸µì—ì„œëŠ” íŒë³„ì  íŠ¹ì§•ì— ì§‘ì¤‘

---

### ğŸ“Œ ì •ë¦¬

| ì¸µ (Layer) | ë¶€ìœ„            | Î” ê°’ ê²°ê³¼       | ì˜ë¯¸                                   |
| ---------- | --------------- | --------------- | -------------------------------------- |
| Layer 5    | ëˆˆÂ·ì½” vs ë¬´ì‘ìœ„ | ëˆˆÂ·ì½” Î” ë” ë‚®ìŒ | ë¶€ìœ„ ê°„ ëŒ€ì‘ì„± í™•ë³´                    |
| Layer 7    | ëˆˆÂ·ì½” vs ë¬´ì‘ìœ„ | ìœ ì‚¬            | breed êµ¬ë¶„ì— ì§‘ì¤‘, correspondence ì•½í™” |
</aside>

### ğŸ“šÂ 5. Experiments

### ë²ˆì—­

1. ì›ë¬¸ (5.1 ImageNet 2012)
    
    This dataset consists of 1.3M/50k/100k training/validation/test examples, spread over 1000 categories. Table 2 shows our results on this dataset.
    
    Using the exact architecture specified in (Krizhevsky et al., 2012), we attempt to replicate their result on the validation set. We achieve an error rate within 0.1% of their reported value on the ImageNet 2012 validation set.
    
    Next we analyze the performance of our model with the architectural changes outlined in Section 4.1 (7Ã—7 filters in layer 1 and stride 2 convolutions in layers 1 & 2). This model, shown in Fig. 3, significantly outperforms the architecture of (Krizhevsky et al., 2012), beating their single model result by 1.7% (test top-5). When we combine multiple models, we obtain a test error of 14.8%, the best published performance on this dataset (despite only using the 2012 training set).
    
    We note that this error is almost half that of the top non-convnet entry in the ImageNet 2012 classification challenge, which obtained 26.2% error.
    

---

1. ë²ˆì—­ë¬¸ (5.1 ImageNet 2012)
    
    ì´ ë°ì´í„°ì…‹ì€ 1000ê°œì˜ ì¹´í…Œê³ ë¦¬ì— ê±¸ì³ 130ë§Œ ì¥ì˜ í•™ìŠµ ì´ë¯¸ì§€, 5ë§Œ ì¥ì˜ ê²€ì¦ ì´ë¯¸ì§€, 10ë§Œ ì¥ì˜ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ëœë‹¤. Table 2ëŠ” ì´ ë°ì´í„°ì…‹ì—ì„œì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤.
    
    ë¨¼ì € Krizhevsky et al. (2012)ì˜ ì•„í‚¤í…ì²˜ë¥¼ ê·¸ëŒ€ë¡œ ì¬í˜„í•˜ì—¬ ê²€ì¦ì…‹ ê²°ê³¼ë¥¼ ë¹„êµí–ˆìœ¼ë©°, ë³´ê³ ëœ ê°’ê³¼ 0.1% ì´ë‚´ë¡œ ì¼ì¹˜í•˜ëŠ” ì˜¤ë¥˜ìœ¨ì„ ë‹¬ì„±í•˜ì˜€ë‹¤.
    
    ë‹¤ìŒìœ¼ë¡œ, Section 4.1ì—ì„œ ì œì•ˆëœ ì•„í‚¤í…ì²˜ ìˆ˜ì •(1ì¸µ í•„í„°ë¥¼ 11Ã—11ì—ì„œ 7Ã—7ë¡œ ì¶•ì†Œ, strideë¥¼ 4ì—ì„œ 2ë¡œ ë³€ê²½)ì„ ì ìš©í•œ ëª¨ë¸ì„ ë¶„ì„í–ˆë‹¤. Fig. 3ì— ë‚˜íƒ€ë‚œ ì´ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ëŠ” Krizhevsky ì•„í‚¤í…ì²˜ë³´ë‹¤ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì–´, ë‹¨ì¼ ëª¨ë¸ ê¸°ì¤€ í…ŒìŠ¤íŠ¸ Top-5 ì˜¤ë¥˜ìœ¨ì„ 1.7% ê°œì„ í–ˆë‹¤.
    
    ë˜í•œ ì—¬ëŸ¬ ëª¨ë¸ì„ ì•™ìƒë¸”í•œ ê²½ìš° í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜ìœ¨ì€ 14.8%ë¡œ ê°ì†Œí–ˆìœ¼ë©°, ì´ëŠ” 2012 í•™ìŠµì…‹ë§Œ ì‚¬ìš©í•œ ëª¨ë¸ ì¤‘ ë‹¹ì‹œ ìµœê³  ì„±ëŠ¥ì´ì—ˆë‹¤. ì´ ì˜¤ë¥˜ìœ¨ì€ ImageNet 2012 ëŒ€íšŒì˜ ë¹„-ConvNet ë°©ë²•(26.2%)ì˜ ì ˆë°˜ ìˆ˜ì¤€ì— í•´ë‹¹í•œë‹¤.
    
2. ì›ë¬¸ (5.2 Feature Generalization, ë…¼ë¬¸ ë³¸ë¬¸ ê·¸ëŒ€ë¡œ)
    
    The experiments above show the importance of the convolutional part of our ImageNet model in obtaining state-of-the-art performance. This is supported by the visualizations of Fig. 2 which show the complex invariances learned in the convolutional layers. We now explore the ability of these feature extraction layers to generalize to other datasets, namely Caltech-101 (Fei-fei et al., 2006), Caltech-256 (Griffin et al., 2006) and PASCAL VOC 2012. To do this, we keep layers 1-7 of our ImageNet-trained model fixed and train a new softmax classifier on top (for the appropriate number of classes) using the training images of the new dataset. Since the softmax contains relatively few parameters, it can be trained quickly from a relatively small number of examples, as is the case for certain datasets.
    
    The classifiers used by our model (a softmax) and other approaches (typically a linear SVM) are of similar complexity, thus the experiments compare our feature representation, learned from ImageNet, with the hand-crafted features used by other methods. It is important to note that both our feature representation and the hand-crafted features are designed using images beyond the Caltech and PASCAL training sets. For example, the hyper-parameters in HOG descriptors were determined through systematic experiments on a pedestrian dataset (Dalal & Triggs, 2005). We also try a second strategy of training a model from scratch, i.e. resetting layers 1-7 to random values and train them, as well as the softmax, on the training images of the dataset.
    
    One complication is that some of the Caltech datasets have some images that are also in the ImageNet training data. Using normalized correlation, we identified these few â€œoverlapâ€ images and removed them from our Imagenet training set and then retrained our Imagenet models, so avoiding the possibility of train/test contamination.
    
    **Caltech-101:** We follow the procedure of (Fei-fei et al., 2006) and randomly select 15 or 30 images per class for training and test on up to 50 images per class reporting the average of the per-class accuracies in Table 4, using 5 train/test folds. Training took 17 minutes for 30 images/class. The pre-trained model beats the best reported result for 30 images/class from (Bo et al., 2013) by 2.2%. The convnet model trained from scratch however does terribly, only achieving 46.5%.
    
    **Caltech-256:** We follow the procedure of (Griffin et al., 2006), selecting 15, 30, 45, or 60 training images per class, reporting the average of the per-class accuracies in Table 5. Our ImageNet-pretrained model beats the current state-of-the-art results obtained by Bo et al. (Bo et al., 2013) by a significant margin: 74.2% vs 55.2% for 60 training images/class. However, as with Caltech-101, the model trained from scratch does poorly. In Fig. 9, we explore the â€œone-shot learningâ€ (Fei-fei et al., 2006) regime. With our pre-trained model, just 6 Caltech-256 training images are needed to beat the leading method using 10 times as many images. This shows the power of the ImageNet feature extractor.
    
    **PASCAL 2012:** We used the standard training and validation images to train a 20-way softmax on top of the ImageNet-pretrained convnet. This is not ideal, as PASCAL images can contain multiple objects and our model just provides a single exclusive prediction for each image. Table 6 shows the results on the test set. The PASCAL and ImageNet images are quite different in nature, the former being full scenes unlike the latter. This may explain our mean performance being 3.2% lower than the leading (Yan et al., 2012) result, however we do beat them on 5 classes, sometimes by large margins.
    
    ---
    

2. ë²ˆì—­ë¬¸ (5.2 íŠ¹ì§• ì¼ë°˜í™”, ì „ì²´)

ì•ì„  ì‹¤í—˜ì€ ImageNet ëª¨ë¸ì˜ í•©ì„±ê³± ë¶€ë¶„ì´ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ë° ì¤‘ìš”í•¨ì„ ë³´ì—¬ì¤€ë‹¤. Fig. 2ì˜ ì‹œê°í™”ëŠ” í•©ì„±ê³± ì¸µì´ ë³µì¡í•œ ë¶ˆë³€ì„±ì„ í•™ìŠµí•¨ì„ ë³´ì—¬ì£¼ë©° ì´ë¥¼ ë’·ë°›ì¹¨í•œë‹¤. ì´ì œ ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ íŠ¹ì§• ì¶”ì¶œ ì¸µì´ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ì„œë„ ì–¼ë§ˆë‚˜ ì˜ ì¼ë°˜í™”ë˜ëŠ”ì§€ë¥¼ íƒêµ¬í•œë‹¤. ì´ë¥¼ ìœ„í•´ ImageNetì—ì„œ í•™ìŠµëœ ëª¨ë¸ì˜ 1~7ì¸µì„ ê³ ì •(freeze)í•˜ê³ , ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì˜ í•™ìŠµ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ ìƒë‹¨ì— ìƒˆë¡œìš´ softmax ë¶„ë¥˜ê¸°ë§Œ í•™ìŠµì‹œì¼°ë‹¤. softmaxëŠ” íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ìƒëŒ€ì ìœ¼ë¡œ ì ìœ¼ë¯€ë¡œ, í•™ìŠµ ì˜ˆì‹œê°€ ì ì€ ë°ì´í„°ì…‹ì—ì„œë„ ë¹ ë¥´ê²Œ í›ˆë ¨í•  ìˆ˜ ìˆë‹¤.

ìš°ë¦¬ ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” softmax ë¶„ë¥˜ê¸°ì™€ ê¸°ì¡´ ì ‘ê·¼ë²•ì´ ì‚¬ìš©í•˜ëŠ” ì„ í˜• SVMì€ ë³µì¡ì„±ì´ ìœ ì‚¬í•˜ë‹¤. ë”°ë¼ì„œ ì´ ì‹¤í—˜ì€ ìš°ë¦¬ê°€ ImageNetì—ì„œ í•™ìŠµí•œ feature í‘œí˜„ê³¼ ê¸°ì¡´ ìˆ˜ì‘ì—… íŠ¹ì§•(hand-crafted features)ì„ ì§ì ‘ ë¹„êµí•˜ëŠ” ê²ƒì´ë‹¤. ì¤‘ìš”í•œ ì ì€, ìš°ë¦¬ì˜ feature í‘œí˜„ê³¼ ìˆ˜ì‘ì—… feature ëª¨ë‘ Caltechê³¼ PASCAL í•™ìŠµì…‹ ì™¸ë¶€ì˜ ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆë‹¤ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, HOG descriptorì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ë³´í–‰ì ë°ì´í„°ì…‹(Dalal & Triggs, 2005)ì—ì„œì˜ ì²´ê³„ì  ì‹¤í—˜ìœ¼ë¡œ ê²°ì •ë˜ì—ˆë‹¤. ë˜í•œ ìš°ë¦¬ëŠ” ë‘ ë²ˆì§¸ ì „ëµìœ¼ë¡œ â€œì²˜ìŒë¶€í„° í•™ìŠµâ€ë„ ì‹œë„í–ˆëŠ”ë°, ì´ëŠ” 1~7ì¸µì„ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”í•˜ê³  softmaxê¹Œì§€ í¬í•¨í•´ ì „ì²´ë¥¼ ìƒˆë¡œ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ë‹¤.

í•œ ê°€ì§€ ë³µì¡ì„±ì€, Caltech ë°ì´í„°ì…‹ ì¼ë¶€ ì´ë¯¸ì§€ê°€ ImageNet í•™ìŠµ ë°ì´í„°ì—ë„ í¬í•¨ë˜ì–´ ìˆë‹¤ëŠ” ì ì´ë‹¤. ìš°ë¦¬ëŠ” ì •ê·œí™”ëœ ìƒê´€(normalized correlation)ì„ ì‚¬ìš©í•´ ì´ëŸ¬í•œ ì¤‘ë³µ ì´ë¯¸ì§€ë¥¼ í™•ì¸í•˜ê³ , ImageNet í•™ìŠµì…‹ì—ì„œ ì œê±°í•œ í›„ ëª¨ë¸ì„ ì¬í•™ìŠµì‹œì¼œ train/test contamination ë¬¸ì œë¥¼ í”¼í–ˆë‹¤.

**Caltech-101:** (Fei-fei et al., 2006)ì˜ ì ˆì°¨ë¥¼ ë”°ë¼ í´ë˜ìŠ¤ë‹¹ 15ì¥ ë˜ëŠ” 30ì¥ì„ ë¬´ì‘ìœ„ë¡œ í•™ìŠµì— ì‚¬ìš©í•˜ê³ , ìµœëŒ€ 50ì¥ì„ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í–ˆë‹¤. 5ê°œì˜ í•™ìŠµ/í…ŒìŠ¤íŠ¸ foldì—ì„œ í´ë˜ìŠ¤ë³„ ì •í™•ë„ì˜ í‰ê· ì„ ë³´ê³ í•˜ì˜€ë‹¤. í´ë˜ìŠ¤ë‹¹ 30ì¥ì˜ ê²½ìš° í•™ìŠµ ì‹œê°„ì€ 17ë¶„ì´ ì†Œìš”ë˜ì—ˆë‹¤. ì‚¬ì „í•™ìŠµ ëª¨ë¸ì€ (Bo et al., 2013)ì´ ë³´ê³ í•œ ìµœê³  ì„±ëŠ¥ì„ 2.2% ëŠ¥ê°€í–ˆìœ¼ë©° (86.5% vs 81.4%), ì²˜ìŒë¶€í„° í•™ìŠµí•œ ConvNetì€ ë§¤ìš° ë‚®ì€ ì„±ëŠ¥(46.5%)ì„ ë³´ì˜€ë‹¤.

**Caltech-256:** (Griffin et al., 2006)ì˜ ì ˆì°¨ë¥¼ ë”°ë¼ í´ë˜ìŠ¤ë‹¹ 15, 30, 45, 60ì¥ì„ í•™ìŠµì— ì‚¬ìš©í–ˆìœ¼ë©°, í´ë˜ìŠ¤ë³„ ì •í™•ë„ì˜ í‰ê· ì„ Table 5ì— ë³´ê³ í•˜ì˜€ë‹¤. ìš°ë¦¬ì˜ ImageNet ì‚¬ì „í•™ìŠµ ëª¨ë¸ì€ ê¸°ì¡´ ìµœê³  ì„±ëŠ¥(Bo et al., 2013)ì˜ 55.2%ë¥¼ í¬ê²Œ ì´ˆê³¼í•˜ì—¬ í´ë˜ìŠ¤ë‹¹ 60ì¥ì¼ ë•Œ 74.2%ë¥¼ ë‹¬ì„±í–ˆë‹¤. ê·¸ëŸ¬ë‚˜ Caltech-101ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì²˜ìŒë¶€í„° í•™ìŠµí•œ ëª¨ë¸ì€ ì„±ëŠ¥ì´ ë§¤ìš° ë‚®ì•˜ë‹¤. Fig. 9ì—ì„œëŠ” â€œone-shot learningâ€ (Fei-fei et al., 2006) ì‹œë‚˜ë¦¬ì˜¤ë¥¼ íƒêµ¬í–ˆëŠ”ë°, ì‚¬ì „í•™ìŠµ ëª¨ë¸ì€ í´ë˜ìŠ¤ë‹¹ ë‹¨ 6ì¥ì˜ í•™ìŠµ ì´ë¯¸ì§€ë¡œë„ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ê¸°ë¡í–ˆë‹¤. ì´ëŠ” ImageNet feature extractorì˜ ê°•ë ¥í•¨ì„ ë³´ì—¬ì¤€ë‹¤.

**PASCAL 2012:** í‘œì¤€ í•™ìŠµ ë° ê²€ì¦ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ImageNet ì‚¬ì „í•™ìŠµ ConvNet ìœ„ì— 20-way softmaxë¥¼ í•™ìŠµì‹œì¼°ë‹¤. ì´ ë°©ì‹ì€ ì´ìƒì ì´ì§€ ì•Šì€ë°, PASCAL ì´ë¯¸ì§€ëŠ” ë‹¤ì¤‘ ê°ì²´ë¥¼ í¬í•¨í•  ìˆ˜ ìˆì§€ë§Œ ëª¨ë¸ì€ ë‹¨ì¼ ë…ì ì  ì˜ˆì¸¡ë§Œ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì´ë‹¤. Table 6ì€ í…ŒìŠ¤íŠ¸ì…‹ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤. PASCALê³¼ ImageNet ì´ë¯¸ì§€ëŠ” ì„±ê²©ì´ ê½¤ ë‹¤ë¥¸ë°, ì „ìëŠ” ì „ì²´ ì¥ë©´ì´ê³  í›„ìëŠ” ë‹¨ì¼ ê°ì²´ ì¤‘ì‹¬ì´ë‹¤. ì´ë¡œ ì¸í•´ í‰ê·  ì„±ëŠ¥ì€ ìµœê³ ì¹˜(Yan et al., 2012)ì˜ 82.2%ë³´ë‹¤ 3.2% ë‚®ì€ 79.0%ì˜€ë‹¤ê³  ì„¤ëª…ëœë‹¤. ê·¸ëŸ¬ë‚˜ 5ê°œ í´ë˜ìŠ¤ì—ì„œëŠ” ì˜¤íˆë ¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ê³ , ë•Œë¡œëŠ” í° ì°¨ì´ë¡œ ì•ì„°ë‹¤.

1. ì›ë¬¸ (5.3 Feature Analysis)
    
    We explore how discriminative the features in each layer of our Imagenet-pretrained model are. We do this by varying the number of layers retained from the ImageNet model and place either a linear SVM or softmax classifier on top. Table 7 shows results on Caltech-101 and Caltech-256. For both datasets, a steady improvement can be seen as we ascend the model, with best results being obtained by using all layers. This supports the premise that as the feature hierarchies become deeper, they learn increasingly powerful features.
    
    **Table 7.** Analysis of the discriminative information contained in each layer of feature maps within our ImageNet-pretrained convnet. We train either a linear SVM or softmax on features from different layers (as indicated in brackets) from the convnet. Higher layers generally produce more discriminative features.
    
    ```
    Cal-101 (30/class)   Cal-256 (60/class)
    
    SVM (1)   44.8 Â± 0.7    24.6 Â± 0.4
    SVM (2)   66.2 Â± 0.5    39.6 Â± 0.3
    SVM (3)   72.3 Â± 0.4    46.0 Â± 0.3
    SVM (4)   76.6 Â± 0.4    51.3 Â± 0.1
    SVM (5)   86.2 Â± 0.8    65.6 Â± 0.3
    SVM (7)   85.5 Â± 0.4    71.7 Â± 0.2
    
    Softmax (5)   82.9 Â± 0.4    65.7 Â± 0.5
    Softmax (7)   85.4 Â± 0.4    72.6 Â± 0.1
    
    ```
    

---

1. ë²ˆì—­ë¬¸ (5.3 íŠ¹ì§• ë¶„ì„)
    
    ìš°ë¦¬ëŠ” ImageNet ì‚¬ì „í•™ìŠµ ëª¨ë¸ì˜ ê° ê³„ì¸µ(feature map)ì´ ì–¼ë§ˆë‚˜ íŒë³„ì (discriminative) ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ”ì§€ë¥¼ íƒêµ¬í•œë‹¤. ì´ë¥¼ ìœ„í•´ ImageNet ëª¨ë¸ì—ì„œ ìœ ì§€í•˜ëŠ” ê³„ì¸µ ìˆ˜ë¥¼ ë‹¬ë¦¬í•˜ê³ , ê·¸ ìœ„ì— ì„ í˜• SVM í˜¹ì€ softmax ë¶„ë¥˜ê¸°ë¥¼ ì–¹ì–´ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ì˜€ë‹¤. Table 7ì€ Caltech-101ê³¼ Caltech-256ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤.
    
    ë‘ ë°ì´í„°ì…‹ ëª¨ë‘, ê³„ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ ì ì§„ì ìœ¼ë¡œ í–¥ìƒë˜ë©°, ëª¨ë“  ê³„ì¸µì„ ì‚¬ìš©í–ˆì„ ë•Œ ìµœê³  ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. ì´ëŠ” **íŠ¹ì§• ê³„ì¸µ(feature hierarchy)ì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ì ì°¨ ê°•ë ¥í•œ íŠ¹ì§•ì„ í•™ìŠµí•œë‹¤**ëŠ” ì „ì œë¥¼ ë’·ë°›ì¹¨í•œë‹¤.
    
    **Table 7.** ImageNet ì‚¬ì „í•™ìŠµ ConvNetì˜ ê° ê³„ì¸µ feature mapì´ ë‹´ê³  ìˆëŠ” íŒë³„ ì •ë³´ ë¶„ì„. ConvNetì˜ ì„œë¡œ ë‹¤ë¥¸ ê³„ì¸µ(ê´„í˜¸ ì•ˆ)ì—ì„œ ì¶”ì¶œí•œ featureì— ëŒ€í•´ ì„ í˜• SVM ë˜ëŠ” softmax ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµí•˜ì˜€ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ìƒìœ„ ê³„ì¸µì´ ë” íŒë³„ì ì¸ íŠ¹ì§•ì„ ìƒì„±í•œë‹¤.
    
    ```
    Cal-101 (30/class)   Cal-256 (60/class)
    
    SVM (1)   44.8 Â± 0.7    24.6 Â± 0.4
    SVM (2)   66.2 Â± 0.5    39.6 Â± 0.3
    SVM (3)   72.3 Â± 0.4    46.0 Â± 0.3
    SVM (4)   76.6 Â± 0.4    51.3 Â± 0.1
    SVM (5)   86.2 Â± 0.8    65.6 Â± 0.3
    SVM (7)   85.5 Â± 0.4    71.7 Â± 0.2
    
    Softmax (5)   82.9 Â± 0.4    65.7 Â± 0.5
    Softmax (7)   85.4 Â± 0.4    72.6 Â± 0.1
    
    ```
    

<aside>

## 5.1. ImageNet 2012

âœ… **5ì¤„ ìš”ì•½**

- ImageNet 2012: í•™ìŠµ 130ë§Œ / ê²€ì¦ 5ë§Œ / í…ŒìŠ¤íŠ¸ 10ë§Œ, 1000 í´ë˜ìŠ¤
- AlexNet êµ¬ì¡° ì¬í˜„ â†’ ë³´ê³ ëœ ì„±ëŠ¥ê³¼ ë™ì¼
- stride 4â†’2, filter 11Ã—11â†’7Ã—7 â†’ ì„±ëŠ¥ í–¥ìƒ
- ë‹¨ì¼ ëª¨ë¸: Top-5 error 1.7% ê°œì„ 
- ì•™ìƒë¸”: 14.8% error â†’ ë‹¹ì‹œ ìµœê³  ì„±ëŠ¥, ë¹„-ConvNetì˜ ì ˆë°˜ ìˆ˜ì¤€

---

ğŸ“Œ **ì •ë¦¬í‘œ (5.1 ImageNet 2012)**

| ëª¨ë¸                      | Top-5 Error (%) | ë¹„ê³                        |
| ------------------------- | --------------- | -------------------------- |
| AlexNet (2012)            | 16.4            | Krizhevsky et al.          |
| Zeiler & Fergus (ë‹¨ì¼)    | ì•½ 14.7â€“15.0    | stride=2, filter=7Ã—7 ì ìš©  |
| Zeiler & Fergus (ì•™ìƒë¸”)  | 14.8            | 2012 í•™ìŠµì…‹ ê¸°ì¤€ ìµœê³  ì„±ëŠ¥ |
| ë¹„-ConvNet (Gunji et al.) | 26.2            | ê°™ì€ ëŒ€íšŒ ìƒìœ„ entry       |

![image.png]((ZFNet)%20Visualizing%20and%20Understanding%20Convolutiona%2025ca9b246de18068af2bc450e4cf0526/image%207.png)

## 5.2 Feature Generalization

### âœ… **5ì¤„ ìš”ì•½**

- ImageNet ì‚¬ì „í•™ìŠµ featureëŠ” ì†Œê·œëª¨ ë°ì´í„°ì…‹ì—ì„œë„ ê°•ë ¥
- Caltech-101: 86.5%, ê¸°ì¡´ ìµœê³ ì¹˜ë³´ë‹¤ +2.2%
- Caltech-256: 74.2%, ê¸°ì¡´ ìµœê³ ì¹˜ë³´ë‹¤ +19%
- PASCAL: í‰ê·  79.0%, ìµœê³ ì¹˜ 82.2%ì— ê·¼ì ‘, ì¼ë¶€ í´ë˜ìŠ¤ëŠ” ìš°ìœ„
- ConvNetì€ ë²”ìš©ì  ì „ì´ í•™ìŠµ ë„êµ¬ì„ì„ ì…ì¦

---

### ğŸ“Œ **ì •ë¦¬í‘œ (5.2 Feature Generalization)**

| ë°ì´í„°ì…‹        | ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì„±ëŠ¥    | ê¸°ì¡´ ìµœê³ ì¹˜ | Scratch í•™ìŠµ | ì˜ë¯¸                                  |
| --------------- | --------------------- | ----------- | ------------ | ------------------------------------- |
| Caltech-101     | 86.5%                 | 81.4%       | 46.5%        | ì†Œê·œëª¨ì—ì„œë„ SOTA                     |
| Caltech-256     | 74.2% (60 imgs/class) | 55.2%       | 38.8%        | ëŒ€ê·œëª¨/ì†Œê·œëª¨ ëª¨ë‘ ì••ë„               |
| PASCAL VOC 2012 | 79.0% (mean)          | 82.2%       | -            | ë‹¤ì¤‘ ê°ì²´ ì¥ë©´, ì¼ë¶€ í´ë˜ìŠ¤ëŠ” ë” ìš°ìœ„ |

## 5.3 Feature Analysis

- **ê¹Šì–´ì§ˆìˆ˜ë¡ íŒë³„ ì„±ëŠ¥ ì¦ê°€**

### âœ… **5ì¤„ ìš”ì•½**

- ConvNetì˜ feature íŒë³„ë ¥ì€ ê¹Šì„ìˆ˜ë¡ í–¥ìƒ
- Layer 1: ì €ì„±ëŠ¥ (ì—£ì§€Â·ìƒ‰ ê¸°ë°˜)
- Layer 5: í° í–¥ìƒ (ì¤‘ê°„ íŠ¹ì§•ì´ ë§¤ìš° ê°•ë ¥)
- Layer 7: Caltech-256ì—ì„œëŠ” ìµœê³ , Caltech-101ì—ì„œëŠ” plateau
- ConvNetì€ ê³„ì¸µì ìœ¼ë¡œ ì ì  ê°•ë ¥í•œ íŠ¹ì§• í‘œí˜„ì„ í•™ìŠµ

---

### ğŸ“Œ **ì •ë¦¬**

| ë°ì´í„°ì…‹    | Layer 1 | Layer 3 | Layer 5 | Layer 7 | ì˜ë¯¸                                        |
| ----------- | ------- | ------- | ------- | ------- | ------------------------------------------- |
| Caltech-101 | 44.8%   | 72.3%   | 86.2%   | 85.5%   | ì¤‘ê°„~ìƒìœ„ì¸µì—ì„œ í° í–¥ìƒ, ìµœìƒìœ„ì¸µì€ plateau |
| Caltech-256 | 24.6%   | 46.0%   | 65.6%   | 71.7%   | ì¸µì´ ê¹Šì„ìˆ˜ë¡ ê³„ì† í–¥ìƒ, ìµœìƒìœ„ì¸µì´ ìµœê°•    |
</aside>

---